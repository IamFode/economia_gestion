\setcounter{chapter}{1}
\chapter{El modelo de regresión simple}
\section{Definición del modelo de regresión simple}
\begin{equation}
    y = \beta_o + \beta_1x + u.
\end{equation}

\section{Obtención de las estimaciones de mínimos cuadrados ordinarios}

Sea $\left\{(x_i,y_i): i = 1,\ldots,n\right\}$, una muestra aleatoria de tamaño $n$ tomada de la población. Como estos datos provienen de $(2.1)$ para todo $i$ puede escribirse 
\begin{equation}
    y_i = \beta_o + \beta_1x_i + u_i.
\end{equation}

En tanto el intercepto $\beta_0$ aparezca en la ecuación, nada se altera al suponer que el valor promedio de $u$ en la población, es cero. Es decir, $E(u)=0$.\\
El supuesto crucial es que el valor promedio de $u$ no depende del valor de $x$. Este supuesto se expresa como $u$ $E(u\backslash x) = E(u)$. Esta última ecuación indica que el valor promedio de los factores no observables es el mismo en todas las fracciones de la población determinados por los valores de $x$ y que este promedio común es necesariamente igual al promedio al promedio de $u$ en toda la población. Y por lo tanto $u$ es media independiente de $x$. Combinando la independencia de la media con el supuesto de $E(u)=0$ se obtiene el supuesto de media condicional cero, $E(u\backslash x) = 0$\\\\

En la población, $u$ no está correlacionada con $x$. Por tanto, se tiene que el valor esperado de $u$ es cero y que la covarianza entre $x$ y $y$ es cero:
\begin{equation}
    E(u)=0
\end{equation}
 y 

\begin{equation}
    Cov(x,u) = E(xu) =0.
\end{equation}

\textbf{Covarianza.-} Sean $\mu_x = E(X)$ y $\mu_y = E(Y)$ y considere la variable aleatoria $(X-\mu_x)(Y-\mu_y)$. Si $X$ es mayor a su media y $Y$ es mayor a su media, entonces $(X-\mu_x)(Y-\mu_y)>0$. La covarianza entre dos variables aleatorias $X$ y $Y$ llamada algunas veces covarianza poblacional, para hacer énfasis en que se refiere a la relación entre dos variables que describen una población, está definida como el valor esperado del producto $(X-\mu_x)(Y-\mu_y)$: 

\begin{equation}
    Cov(X,Y) = E\left[(X-\mu_x)(Y-\mu_y)\right]
\end{equation}

que también suele denotarse como $\sigma_{XY}.$  \\
Algunas expresiones para útiles para calcular $Cov(X,Y)$ son las siguientes

\begin{equation}
    Cov(X,Y) = E\left[(X-\mu_x)(Y-\mu_y)\right] = E\left[(X-\mu_X)Y\right] = E\left[X(Y-\mu_Y)\right] = E(XY) - \mu_x \mu_y
\end{equation}

De donde se sigue que si $E(X)=0$ o $E(Y)=0$, entonces $Cov(X,Y) = E(XY)$.\\\\

Luego  
\begin{equation}
    E(y - \beta_0 - \beta_1x) = 0
\end{equation}
 
y 

\begin{equation}
    E\left[x(y - \beta_0 -\beta_i x )\right] = 0.
\end{equation}

Como hay que estimar dos parámetros desconocidos, se espera que las dos ecuaciones anteriores puedan servir para obtener buenos estimadores de $\beta_0$ y $\beta_1$. En efecto estas ecuaciones pueden servir para la estimación de estos parámetros.\\\\

Por el método de momentos para la estimación , y las anteriores dos ecuaciones,
\begin{equation}
    n^{-1} \sum\limits_{i=1}^n (y_i - \beta_0 - \beta_1x_i) = 0
\end{equation}
y 
\begin{equation}
    n^{-1} \sum\limits_{i=1}^n x_i(y_i - \beta_0 - \beta_1x_i) = 0
\end{equation}
también llamada \textbf{condiciones de primer orden para los estimadores de MCO}. Luego por la ecuación (2.9) tenemos que 
\begin{equation}
    \overline{y} = \hat{\beta_0} + \hat{\beta_1}\overline{x}.
\end{equation}
donde $\overline{y} = n^{-1}\sum\limits_{i=1}^n y_i$ es el promedio muestral de las $y_i$, y lo mismo ocurre con $\overline{x}$, así, 

\begin{tcolorbox}[colframe=white]
\begin{equation}
    \hat{\beta_0} = \overline{y} - \hat{\beta_1}\overline{x}.
\end{equation}
\end{tcolorbox}
Por último empleando (2.10) y (2.12) para sustituir $\hat{\beta_0}$ se obtiene,
$$\sum_{i=1}^n x_i\left[y_i-(\overline{y}-\hat{\beta_1}\overline{x})-\hat{\beta_1}x_i\right] = 0,$$
de donde, reordenando, tenemos que
$$\sum_{i=1}^n x_i(y_i-\overline{y}) = \hat{\beta_1}\sum_{i=1}^n x_i(x_i-\overline{x}).$$
en consecuencia por las propiedades de la sumatoria, 
$$\sum_{i=1}^n x_i(x_i-\overline{x}) = \sum_{i=1}^n (x_i-\overline{x})^2 \quad y \quad \sum_{i=1}^n x_i(y_i-\overline{y}) = \sum_{i=1}^n (x_i-\overline{x})(y_i-\overline{y})$$\\
ya que $\sum\limits_{i=1}^n (x_i-\overline{x})(y_i-\overline{y})=\sum\limits_{i=1}^n x_i y_i - \overline{y}\sum\limits_{i=1}^n x_i - \overline{x}\sum\limits_{i=1}^n y_i + \overline{y} \overline{x}\sum\limits_{i=1}^n 1 $, luego ya que $\sum\limits_{i=1}^n x_i = n\overline{x}$ entonces $\sum\limits_{i=1}^n x_iy_i - n\overline{yx} - n\overline{xy} + n\overline{xy} = \sum\limits_{i=1}^n x_i(y_i-\overline{y})$
por lo tanto, 
\begin{tcolorbox}[colframe=white]
\begin{equation}
    \hat{\beta_1} = \frac{\sum\limits_{i=1}^n (x_i-\overline{x})(y_i-\overline{y})}{\sum\limits_{i=1}^n (x_i-\overline{x})^2}.
\end{equation}
\end{tcolorbox}
Ésta ecuación no es nada mas que la covarianza muestral en $x$ e $y$ dividida entre la variación muestral de $x$. Esto tiene sentido porque $\beta_1$ es igual a la covarianza poblacional dividida entre la varianza de $x$ cuando $E(u)=0$ y $Cov(x,u) = 0$. Como consecuencia directa se tiene que si en la muestra $x$ e $y$ están correlacionadas positivamente, entonces $\hat{\beta_1}$ es positiva y contrariamente.\\\\
Para todo $\hat{\beta_0}$ y $\hat{\beta_1}$ se define el valor ajustado para $y$ cuando $x=x_i$ como 
\begin{equation}
	\hat{y_i} = \hat{\beta_0} + \hat{\beta_1}x_i.
\end{equation}
Este es el valor que se predice para $y$ cuando $x=x_i$.\\\\
El \textbf{residual} de la observación $i$ es la diferencia entre el verdadero valor $y_i$ y su valor ajustado.\begin{equation} 
    \hat{u} = y_i - \hat{y_i} = y_i - \hat{\beta_0} - \hat{\beta_1}x_i.
\end{equation}
Los residuales no son lo mismo que la ecuación (2.2)\\\\
Supongamos que $\hat{\beta_0}$ y $\hat{\beta_1}$, se eligen de manera que la suma de residuales cuadrados, 
\begin{equation}
    \sum\limits_{i=1}^n \hat{u_i}^2 = \sum\limits_{i=1}^n (y_i - \hat{\beta_0} - \hat{\beta_1}x_i)^2
\end{equation}
sea tan pequeña como sea posible. \\\\

\textbf{Minimización de la suma de los residuos cuadrados.-}
Se mostrará que $\hat{\beta_0}$ y $\hat{\beta_1}$ estimados de MCO minimizan la suma de los residuales cuadrados. Formalmente, el problema es encontrar las soluciones $\hat{\beta_0}$ y $\hat{\beta_1}$ del problema de minimización 
$$\min_{b_0,b_1} \sum_{i=1}^n (y_i-b_0-b_ix_i)^2$$
donde $b_0$ y $b_1$ son argumentos ficticios en el problema de optimización. Para simplificar llámesele a esta función $Q(b_0,b_1)$. Una condición para que $\hat{\beta_0}$ y $\hat{\beta_1}$ sean soluciones del problema de minimización es que las derivadas parciales de $Q(b_0,b_1)$ respecto a $b_0$ y $b_1$ evaluadas en $$\hat{\beta_0},\hat{\beta_1}: \quad \dfrac{\partial Q(\hat{\beta_0},\hat{\beta_1})}{\partial b_0}=0 \qquad y \qquad \dfrac{\partial Q(\hat{\beta_0},\hat{\beta_1})}{\partial b_1}=0$$
Con ayuda de la regla de la cadena del cálculo, estas dos ecuaciones se convierten en,
$$-2\sum_{i=1}^n \hat{u} = -2\sum_{i=1}^n(y_i-\hat{\beta}_0 - \hat{\beta}_1 x_i)=0, \qquad -2\sum_{i=1}^n x_1\hat{u} = -2\sum_{i=1}^n x_i(y_i-\hat{\beta}_0 - \hat{\beta}_1 x_i)=0$$
Una manera de comprobar que se ha minimizado la suma de los residuos cuadrados es expresando, para cualquier $b_0$ y $b_1$,
$$\begin{array}{rcl}
    Q(b_0,b_1)&=&\sum\limits_{i=1}^n \left[y_i-\hat{\beta}_0 - \hat{\beta}_1 + (\hat{\beta}_0 - \hat{\beta}_1 - b_0) + (\hat{\beta}_i - b_1)x_i\right]^2\\\\
	      &=&\sum\limits_{i=1}^n \left[\hat{u}_i + (\hat{\beta}_0 - b_0) + (\hat{\beta}_i - b_1)x_i\right]^2\\\\
	      &=&\sum\limits_{i=1}^n \hat{u}_1^2 + n(\hat{\beta}_0-b_0)^2 + (\hat{\beta}_1-b_1)\sum\limits_{i=1}^n x_i^2 + 2(\hat{\beta}_0-b_0)(\hat{\beta}_1-b_1)\sum\limits_{i=1}^n x_i\\\\
\end{array}$$
Luego ya que el primer termino no depende de $b_0$ ni de $b_1$ entonces,
$$\sum_{i=1}^n \left[(\hat{\beta}_0-b_0)+(\hat{\beta}_1-b_1)x_i\right]^2$$
Dado que ésta es una suma de términos al cuadrado, el menor valor que puede tener es 0. Por lo tanto, tendrá el menor valor posible cuando $b_o=\hat{\beta}_0$ y $b_1=\hat{\beta}_1$.\\\\

Con MCO se podrá obtener insesgamiento, consistencia y otras propiedades estadísticas de una manera relativamente sencilla. Además, como sugiere la motivación en las ecuaciones (2.8) y (2.9)  y como se verá en la sección 2.5, el método de MCO es adecuado para estimar los parámetros que aparecen en la función de la media condicional.\\\\

Una vez que se han determinado las estimaciones por MCO del intercepto y de la pendiente,
se obtiene la \textbf{línea de regresión de MCO o función de regresión muestral (FRM)}:

\begin{tcolorbox}[colframe=white]
    \begin{equation}
	\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x.
    \end{equation}
\end{tcolorbox}

Donde se entiende que $\hat{\beta}_0$ y $\hat{\beta}_1$ han sido obtenidas empleando las ecuaciones (2.12) y (2.13). La notación $\hat{y}$ que se lee $y \; gorro$ indica los valores predichos por la ecuación (2.17) son estimaciones. \\\\
En la mayoría de los casos, la pendiente estimada, se puede expresar como,
\begin{equation}
    \hat{\beta}_1 = \dfrac{\triangle \hat{y}}{\triangle x}
\end{equation}
es de mayor interés, pues indica la cantidad en la que cambia $\hat{y}$ cuando $x$ se incrementa en una unidad. De manera equivalente,
\begin{equation}
    \hat{y} =  \hat{\beta}_1 \triangle x.
\end{equation}

\section{Propiedades de MCO en cualquier muestra de datos}
El residual de MCO correspondiente a la observación $i$, $\hat{u}_i$, es la diferencia entre $y_i$ y su valor ajustado, como se indica en la ecuación (2.15). Si $\hat{u}_i$ es positivo, la línea predice un valor que subestima al de $y_i$; si $\hat{u}_i$ es negativo, la linea predice un valor en exceso al de $y_i$. Lo ideal para la observación $i$ es cuando $\hat{u}_i = 0$, pero en la mayoría de los casos todos los residuales son distintos de cero. 

\subsection{Propiedades algebraicas de los estadísticos de MCO}
Las estimaciones de $MCO$ y sus correspondientes estadísticos tiene varias propiedades útiles. A continuación se verán las tres más importantes.
\begin{enumerate}[\bfseries 1.]

    \item La suma y por tanto el promedio muestral de los residuales de MCO, es cero. Matemáticamente
	\begin{equation}
	    \sum_{i=1}^n \hat{u}_i = 0.
	\end{equation}

    \item La covarianza muestral entre los regresores y los residuales de MCO es cero. Esto es consecuencia de la condición de primer orden (2.10) que en términos de los residuales puede expresarse como
    	\begin{equation}
	    \sum_{i=1}^n x_i \hat{u}_i = 0.
	\end{equation}

	El promedio muestral de los residuos de MCO, es cero, por lo que el lado izquierdo de la ecuación anterior es proporcional a la covarianza entre las $x_i$ y los $\hat{u}_i$.

    \item El punto $(\overline{x},\overline{y})$ se encuentran siempre sobre la línea de regresión de MCO. En otras palabras, si en la ecuación $\hat{y} = \hat{\beta}_i x + \hat{\beta}_o$ se sustituye $\overline{x}$ por $x$, el valor predicho es $\overline{y}$. Esto es exactamente lo que dice la ecuación $\overline{y} = \hat{\beta}_0 + \hat{\beta}_1 \overline{x}$.\\\\
\end{enumerate}

Sea 
\begin{equation}
    y_i = \hat{y}_i + \hat{u}_i.
\end{equation}
Por la propiedad 1, el promedio de los residuos es cero, equivalentemente, el promedio muestral de los valores ajustados $\hat{y}_i$, es el mismo que el promedio muestral de $y_i$ o $\overline{\hat{y}} = \overline{y}$\\
además por la propiedad 1 y 2  muestra que la covarianza muestral entre $\hat{y}$ y $\hat{u}$ es cero. Entonces podemos ver que a $MCO$  como la descomposición de $y_i$ en dos partes, un valor ajustado y un residual. De los cuales no están correlacionados en la muestra.\\\\

Definamos el \textbf{total de la suma de los cuadrados}\\

\begin{equation}
    SST = \sum\limits_{i=1}^n (y_i - \overline{y})^2
\end{equation}
SST es una medida de la variación total de la muestra en $y_i$; es decir, mide qué tan dispersos están los $y_i$ en la muestra. Si dividimos por $n-1$ obtendremos la varianza muestral de $y$\\\\

luego el \textbf{Suma Explicada de cuadrados}\\

\begin{equation}
    SSE = \sum\limits_{i=1}^n (\hat{y}_i - \overline{y})^2
\end{equation}
SSE mide la variación muestral en $\hat{y}_i$ (donde usamos el hecho de que $\overline{\hat{y}} = \overline{y}$\\\\ 
Y la \textbf{suma de cuadrados residuales (SSR)}

\begin{equation}
    SSR = \sum\limits_{i=1}^n \hat{u}_i^2 
\end{equation}
SSR mide la variación muestral en $\hat{u}_i$\\\\

La variación total en $y$ puede expresarse como la suma de las variaciones explicativas y la variación no explicativa SSR, 
\begin{equation}
	SST = SSE + SSR
\end{equation}
Demostración.-\; 
$$\begin{array}{rcl}
    \sum\limits_{i=1}^n (y_i - \overline{y})^2&=&\sum\limits_{i=1}^n\left[(y_i - \hat{y})+(\hat{y}_i - \overline{y})\right]^2\\\\
					      &=&\sum\limits_{i=1}^n \left[\hat{u}_i + (\hat{y}_i - \overline{y})\right]^2 \\\\
					      &=&\sum\limits_{i=1}^n \hat{u}_i + 2\sum\limits_{i=1}^n\hat{u}(\hat{y}-\overline{y}) + \sum\limits_{i=1}^n (\hat{y}_i - \overline{y})^2\\\\
					      &=&SSR + \sum\limits_{i=1}^n \hat{u}_i (\hat{y}_i - \overline{y}) + SSE\\\\
\end{array}$$
Ahora (2.36) es verdadera si se muestra que 
$$\sum_{i=1}^n \hat{u}_i (\hat{y}_i - \overline{y}) = 0$$
Pero ya se a dicho que la covarianza mueestral entre los residuos y los valores ajustados es vero y esta covarianza es precisamente (2.37) dividida entre $n-1$.\\\\

\subsection{Bondad de ajuste}
Hasta ahora, no tenemos forma de medir qué tan bien la variable explicativa o independiente, $x$, explica la variable dependiente $y$. Para ello dividimos (2.26) por $SST$ para que nos quede $1=SSE/SST + SSR/SST$. EL r cuadrado de la regresión, a veces es llamado \textbf{coeficiente de determinación} y es definido por,
\begin{equation}
	R^2 = SSE/SST = 1 - SSE/SST
\end{equation}

$R^2$ es el razón de la variación explicada en comparación con la variación total. Por tanto, se interpreta como la fracción de la variación muestral en $y$ que se explica por $x$. Usualmente el resultado es multiplicado por $100$ para que nos de en un porcentaje.\\
\textbf{Si todos los puntos de datos se encuentran en la misma línea, MCO proporciona un ajuste perfecto a los datos. En este caso, $R_2 = 1$. Un valor de $R_2$ que es casi igual a cero indica un mal ajuste de la línea MCO}.\\
De hecho, se puede demostrar que $R_2$ es igual al cuadrado del coeficiente de correlación muestral entre $y_i$ y $hat{y}^i$.\\\\
Vale la pena enfatizar ahora que un R cuadrado aparentemente bajo no significa necesariamente que una ecuación de regresión MCO sea inútil. Tenga en cuenta que el uso de R-cuadrado como principal indicador del éxito de un análisis econométrico puede generar problemas.\\\\

\setcounter{section}{4}
\section{Valores esperados y varianzas de los estimadores de MCO}
Ahora veremos a $\hat{\beta}_0$ y $\hat{\beta}_1$ como estimadores de los parametros $\beta_0$ y $\beta_1$

\subsection{Insesgamiento de MCO}

\begin{tcolorbox}[title={Supuesto 1 SLR.1. Liniealidad en los parametros},colback = white]
    En el modelo poblacional, la variable dependiente $y$ es relacionado con la variable independiente $x$ y el error (o perturbación) $u$ como 
    \begin{equation}
	y = \beta_0 + \beta_1 x + u
    \end{equation}
\end{tcolorbox}

\begin{tcolorbox}[title={Supuesto 2 SLR.2. Muestro aleatorio},colback = white]
    tenemos una variable aleatoria de tamaño $n$, $\lbrace(x_i,y_i): i = 1,2,\ldots,n\rbrace$, siguiendo un modelo poblacional de (2.28)
\end{tcolorbox}

Podemos escribir 2.28 en terminos de variable aleatoria, como 
\begin{tcolorbox}[colframe=white]
    \begin{equation}
	y_i = \beta_0 + \beta_1 x_i + \epsilon_i
    \end{equation}
\end{tcolorbox}

\begin{tcolorbox}[title={Supuesto 3 SLR.3. Variación aleatoria en la variable explicativa},colback = white]
    Los resultados de la muestra en $x$, es decir, $\lbrace x_i,i=1,\ldots, n\rbrace$, no todos tienen el mismo valor.
\end{tcolorbox}

\begin{tcolorbox}[title={Supuesto 4 SLR.4. Media condicional cero},colback = white]
    El error $u$ tiene una valor esperado cero dado cualquier valor de la variable explicativa, en otras palabras,
    $$E(u|x) = 0$$
    Para una muestra aleatoria, este supuesto implica que $E(u_i|x_i), \; \forall i = 1,2,,\ldots,n.$
\end{tcolorbox}

Ahora tomemos el hecho de que $\sum\limits_{i=1}^n (x_i-\overline{x})(y_i-\overline{y}) = \sum\limits_{i=1}^n (x_i-\overline{x})y_i$
Esto se demuestra de la siguiente manera:\\\\
Sea $$\sum\limits_{i=1}^n x_i \overline{y} = \sum\limits_{i=1}^n \overline{x}y_i = n(\overline{x}\overline{y})$$
ya que $\overline{y}\sum\limits_{i=1}^n x_i  \frac{n}{n}\;$ o $\;\overline{x}\sum\limits_{i=1}^n y_i  \frac{n}{n}$ entonces 
$$\begin{array}{rcl}
    \sum\limits_{i=1}^n (x_i-\overline{x})(y_i-\overline{y})&=&\sum\limits_{i=1}^n x_iy_i - \sum\limits_{i=1}^n x_i\overline{y} - \sum\limits_{i=1}^n \overline{x}y_i + \sum\limits_{i=1}^n \overline{x}\overline{y}\\\\
							    &=&\sum\limits_{i=1}^n x_iy_i - n(\overline{x}\overline{y}) - n(\overline{x}\overline{y})+n(\overline{x}\overline{y})\\\\
							    &=&\sum\limits_{i=1}^n x_iy_i - n(\overline{x}\cdot \overline{y})\\\\
							    &=&\sum\limits_{i=1}^n x_iy_i - \sum\limits_{i=1}^n \overline{x}\cdot \overline{y}\\\\
							    &=&\sum\limits_{i=1}^n \left( x_iy_i - \overline{x}\cdot \overline{y}\right)\\\\\
							    &=&\sum\limits_{i=1}^n x_i(y_i-\overline{y}) \;\; o \; \; \sum\limits_{i=1}^n(x_i-\overline{x})y_i\\\\
\end{array}$$

Luego escribimos el estimador de la pendiente de $MCO$ como sigue: 
\begin{equation}
    \hat{\beta}_1 = \dfrac{\sum\limits_{i=1}^n (x_i-\overline{x})y_i}{\sum\limits_{i=1}^n (x_i-\overline{x})^2}
\end{equation}

Ahora consideremos a $\hat{\beta}_1$ como una variable aleatoria.\\\\
Sustituyendo la ecuación $y_i = \beta_0 + \beta_1 x_i + u_i$ dentro (2.30) tenemos que 


\begin{equation}
    \hat{\beta}_1 = \dfrac{\sum\limits_{i=1}^n (x_i-\overline{x})y_i}{\sum\limits_{i=1}^n (x_i-\overline{x})^2} = \dfrac{\sum\limits_{i=1}^n (x_i-\overline{x})(\beta_0 + \beta_1 x_i + u_i)}{SST_x} = \dfrac{\beta_0\sum\limits_{i=1}^n (x_i-\overline{x}) + \beta_1\sum\limits_{i=1}^n (x_i - \overline{x})x_i + \sum\limits_{i=1}^n (x_i-\overline{x})u_i}{SST_x} 
\end{equation}

Dado que $\sum\limits_{i=1}^n (x_i-\overline{x}) = 0$ y $\sum\limits_{i=1}^n (x_i-\overline{x})x_i = \sum\limits_{i=1}^n (x_i-\overline{x})^2 = SST_x$ 
Entonces, podemos escribir el numerador de $\hat{\beta}_1$ como $\beta_1 \; SST_x + \sum\limits_{i=1}^n(x_i-\overline{x})u_i$. Luego
\begin{equation}
    \hat{\beta}_1 = \beta_1 + \dfrac{\sum\limits_{i=1}^n (x_i-\overline{x})u_i}{SST_x} = \beta_1 + \dfrac{\sum\limits_{i=1}^n (x_i-\overline{x})u_i}{\sum\limits_{i=1}^n (x_i - \overline{x})^2} = \beta_1 + (1/SST_x) \sum\limits_{i=1}^n d_i u_i
\end{equation}
donde $d_i = x_i - \overline{x}$.\\\\

Ahora vemos que el estimador $\hat{\beta}_1$ es igual a la pendiente de la población $\beta_1$, más un termino que es una combinación lineal en los errores $[u_1,u_2,\ldots,u_n]$. Condicional a los valores de $x_i$, la aleatoriedad en $\hat{\beta}_1$ se debe enteramente a los errores en la muestra. El hecho de que estos errores sean generalmente diferentes de cero es lo que hace que $\hat{\beta}_1$ difiera de $\beta_1$. Usando la representación en (2.32), podemos probar la primera propiedad estadística importante de MCO.\\

%teorema 2.1
\begin{teo}[Insesgamiento de los estimadores de MCO]
    Usando los supuestos SRL.1 al SRL.4
    \begin{equation}
	E(\hat{\beta}_0)  = \beta_0 \quad y \quad E(\hat{\beta}_1) = \beta_1
    \end{equation}
    Para cuaquier valore de $\beta_0$ y $\beta_1$. $\hat{\beta}_0$ es un estimador insesgado por $\beta_0$ y $\hat{\beta}_1$ es un estimador insesgado por $\beta_1$.\\\\
    Demostración.-\; En esta demostración, los valores esperados son condicionales sobre los valores muestrales de la variable independiente.  Porque $SST_x$ y $d_i = x_i - \overline{x}$  son funciones solo de $x_i$, estos son no aleatorios bajo el condicionamiento. Luego por (2.32) y manteniendo el condicional sobre $\lbrace x_1,x_2,\ldots,x_n\rbrace$ implícito, tenemos 
    $$E(\hat{\beta}_1) = \beta_1 + E\left[(1/SST_x)\sum_{i=1}^n d_i u_i\right] = \beta_1 + (1/SST_x)\sum_{i=1}^n E(d_iu_i) = \beta_1 + (1/SST_x) \sum_{i=1}^n d_i E(U_i) = \beta_1 + (1/SST_x) \sum_{i=1}^n d\cdot 0 = \beta_1$$
    Donde tenemos que usar el hecho de que el valor esperado de cada $u_i$ (condiciones sobre $\lbrace x_i,x_2,\ldots,x_n \rbrace$) es cero bajo los supuestos de SLR.2 y SLR.4. Dado que el insesgamiento es válido para cualquier resultado sobre $\lbrace x_1,x_2,\ldots,x_n \rbrace$, la insesgadez también es válida sin el concionamiento sobre $\lbrace{x_1,x_2,\ldots , x_n\rbrace}$.\\
    La prueba para $\hat{\beta}_0$ es ahora simple. Se promedia (2.28) sobre todas las $i$ y se obtiene $\overline{y} = \beta_0 + \beta_1 \overline{x} + \overline{u}$ y se sustituye en la fórmula para $\hat{\beta}_0$
    $$\hat{\beta}_0 = \hat{\beta_1}\overline{x} = \beta_0 + \beta_1 \overline{x} + \overline{u} - \hat{\beta}_1 \overline{x} = \beta_0 + (\beta_1 - \hat{\beta}_1)\overline{x} + \overline{u}$$
    se sigue que condicionado sobre los valores de las $x_i$
    $$E(\hat{\beta}_0) = \beta_0 + E\left[\beta_1 -\hat{\beta}_1 \overline{x}\right] + E(\overline{u}) = \beta_0 + E\left[\beta_1 - \hat{\beta_1}\right]\overline{x}$$
    ya que $E(\overline{u}) = 0$ de acuerdo con los supuestos de SLR.2 y SLR.4. Pero, ya se mostró que $E(\beta_1) = \beta_1$, lo que implica que $E\left[\hat{\beta_1}-\beta_1\right]=0$. Por lo tanto, $E(\hat{\beta}_0) = \beta_0$. Estos dos argumentos son válidos para todos los valores de $\beta_0$ y $\beta_1$ con lo que queda demostrado el insesgamiento.

\end{teo}

\subsection{Varianza de los estimadores de MCO}
Se agregará un supuesto tradicional en el análisis de corte transversal y que establece que la varianza de los factores inobservables $u$, condicionales en $x$, es constante. Esto se conoce como el supuesto de homocedasticidad o de varianza constante.\\\\


\begin{tcolorbox}[title={Supuesto RLS.5. homocedasticidad},colback = white]
    El error $u$ tiene la misma varianza para cualquier valor de la variable explicativa. En otras palabras 
    $$Var(u|x) = \sigma^2$$
\end{tcolorbox}

Se debe señalar que el supuesto de homocedasticidad es totalmente distinto al de media condicional cero, $E(u|x) = 0$. Añadimos el supuesto SLR.5 porque simplifica los cálculos para $\hat{\beta}_0$ y $\hat{\beta}_1$ y porque implica que los mínimos cuadrados ordinarios tienen ciertas propiedades de eficiencia.\\\\ 
Si se supone que $u$ y $x$ son independientes, entonces la distribución de $u$ dada $x$ no depende de $x$, entonces $E(u|x) = E(u) = 0$ y $Var(u|x) = 0 = \sigma^2$. Pero la independencia es algunas veces muy fuerte de suponer.\\
Ya que $Var(x|u) = E(u^2|x) - E^2(u|x)$ y $E(u|x) = 0$, $\sigma^2 = E(u^2|x)$, el cual significa que $\sigma^2$  es también esperanza incondicional de $u^2$. Por lo tanto, $\sigma^2 = E(u^2) = Var(u)$ por que $E(u) = 0$. En otras palabras, $\sigma^2$ es la varianza incondicional de $u$ y $\sigma^2$ a menudo es llamado el \textbf{la varianza del error} o varianza de la perturbación. La raíz cuadrada de $\sigma^2$ es $\sigma$ la desviación estándar del error. Una $\sigma$ mayor, indica que la distribución de los factores inobservables que afectan a $y$ tiene una mayor dispersión.\\\\

A menudo es útil escribir los supuestos SRL.4 y SLR.5 en términos de la media condicional y la varianza condicional de $y$:
\begin{equation}
    E(y|x) = \beta_0 + \beta_1 x + u
\end{equation}

\begin{equation}
    Var(y|x) = \sigma^2
\end{equation}

En otras palabras la esperanza condicional de $y$ dado $x$ es lineal en $x$, pero la varianza de $y$ dado $x$ es constante. \\
Cuando $Var(u|x) $ depende de $x$ se dice que el término de error exhibe heterocedasticidad o varianza no constante. Como $Var(u|x) = Var(y|x)$, la heterocedasticidad está presente siempre que $Var(y|x)$ sea función de $x$.\\\\
Una vez que se asume la homocedasticidad se está listo para establecer lo siguiente,\\

\begin{teo}[Varianza de muestro de los estimadores de MCO]
    Bajo los supuestos SLR.1 al SLR.5
    \begin{equation}
	Var(\hat{\beta}_1) = \dfrac{\sigma^2}{\sum\limits_{i=1}^n (x_i-\overline{x})^2} = \sigma^2 / SST_x,
    \end{equation}
    y
    \begin{equation}
	Var(\hat{\beta}_0) = \dfrac{\sigma^2 n^{-n} \sum\limits_{i=1}^n x_i^2}{\sum\limits_{i=1}^n (x_i-\overline{x})^2}
    \end{equation}
    Donde estos son condicionales sobre los valores muestrales $\lbrace x_1,\ldots , x_n\rbrace$\\\\
    Demostracíón.-\; Deduciremos la fórmula para $Var(\hat{\beta}_1)$, dejando la otra como ejercici0 2.10. Nuestro punto de partida será (2.32).
    $$\hat{\beta}_1 = \beta_1 + (1/SST_x) \sum\limits_{i=1}^n d_i u_i$$. Como $\beta_1$ es constante y que se está condicionando sobre las $x_i$, $SST_x$ y $d_i = x_i-\overline{x}$ también son no aleatorias. Además, como las $u_i$ son variables aleatorias independientes sobre todos los $i$ (por muestro aleatorio), la varianza es la suma de las varianzas. Usando este hecho tenemos,
    $$\begin{array}{rcl}
	Var(\hat{\beta}_1)&=&(1/SST_x)^2 Var\left(\sum\limits_{i=1}^n d_iu_i\right)  \\\\
			  &=&(1/SST_x)^2 \left[\sum\limits_{i=1}^n d_i^2 Var(u_i)\right]\\\\
			  &=&(1/SST_x)^2 \left(\sum\limits_{i=1}^n d_i^2 \sigma^2\right) \\\\
			  &=&\sigma^2 (1/SST_x)^2\left(\sum\limits_{i=1}^nd_i^2\right)\\\\
			  &=&\sigma^2 (1/SST_x)^2 SST_x\\\\
			  &=&\sigma^2/SST_x\\\\
    \end{array}$$
    Lo cual es lo que se quería demostrar.
\end{teo}

Las ecuaciones (2.36) y (2.37) son fórmulas estándar para el análisis de regresión simple y no son válidas en presencia de heterocedasticidad. Esto será importante cuando se estudien los intervalos de confianza y las pruebas de hipótesis en el análisis de regresión simple.

\subsection{Estimación de la varianza de error}

