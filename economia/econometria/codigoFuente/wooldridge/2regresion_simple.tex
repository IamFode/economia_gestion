\setcounter{chapter}{1}
\chapter{El modelo de regresión simple}
\section{Definición del modelo de regresión simple}
\begin{equation}
    y = \beta_o + \beta_1x + u.
\end{equation}

\section{Obtención de las estimaciones de mínimos cuadrados ordinarios}

Sea $\left\{(x_i,y_i): i = 1,\ldots,n\right\}$, una muestra aleatoria de tamaño $n$ tomada de la población. Como estos datos provienen de $(2.1)$ para todo $i$ puede escribirse 
\begin{equation}
    y_i = \beta_o + \beta_1x_i + u_i.
\end{equation}

En tanto el intercepto $\beta_0$ aparezca en la ecuación, nada se altera al suponer que el valor promedio de $u$ en la población, es cero. Es decir, $E(u)=0$.\\
El supuesto crucial es que el valor promedio de $u$ no depende del valor de $x$. Este supuesto se expresa como $u$ $E(u\backslash x) = E(u)$. Esta última ecuación indica que el valor promedio de los factores no observables es el mismo en todas las fracciones de la población determinados por los valores de $x$ y que este promedio común es necesariamente igual al promedio al promedio de $u$ en toda la población. Y por lo tanto $u$ es media independiente de $x$. Combinando la independencia de la media con el supuesto de $E(u)=0$ se obtiene el supuesto de media condicional cero, $E(u\backslash x) = 0$\\\\

En la población, $u$ no está correlacionada con $x$. Por tanto, se tiene que el valor esperado de $u$ es cero y que la covarianza entre $x$ y $y$ es cero:
\begin{equation}
    E(u)=0
\end{equation}
 y 

\begin{equation}
    Cov(x,u) = E(xu) =0.
\end{equation}

\textbf{Covarianza.-} Sean $\mu_x = E(X)$ y $\mu_y = E(Y)$ y considere la variable aleatoria $(X-\mu_x)(Y-\mu_y)$. Si $X$ es mayor a su media y $Y$ es mayor a su media, entonces $(X-\mu_x)(Y-\mu_y)>0$. La covarianza entre dos variables aleatorias $X$ y $Y$ llamada algunas veces covarianza poblacional, para hacer énfasis en que se refiere a la relación entre dos variables que describen una población, está definida como el valor esperado del producto $(X-\mu_x)(Y-\mu_y)$: 

\begin{equation}
    Cov(X,Y) = E\left[(X-\mu_x)(Y-\mu_y)\right]
\end{equation}

que también suele denotarse como $\sigma_{XY}.$  \\
Algunas expresiones para útiles para calcular $Cov(X,Y)$ son las siguientes

\begin{equation}
    Cov(X,Y) = E\left[(X-\mu_x)(Y-\mu_y)\right] = E\left[(X-\mu_X)Y\right] = E\left[X(Y-\mu_Y)\right] = E(XY) - \mu_x \mu_y
\end{equation}

De donde se sigue que si $E(X)=0$ o $E(Y)=0$, entonces $Cov(X,Y) = E(XY)$.\\\\

Luego  
\begin{equation}
    E(y - \beta_0 - \beta_1x) = 0
\end{equation}
 
y 

\begin{equation}
    E\left[x(y - \beta_0 -\beta_i x )= 0\right]
\end{equation}

Como hay que estimar dos parámetros desconocidos, se espera que las dos ecuaciones anteriores puedan servir para obtener buenos estimadores de $\beta_0$ y $\beta_1$. En efecto estas ecuaciones pueden servir para la estimación de estos parámetros.\\\\

Por el método de momentos para la estimación , y las anteriores dos ecuaciones,
\begin{equation}
    n^{-1} \sum\limits_{i=1}^n (y_i - \beta_0 - \beta_1x_i) = 0
\end{equation}
y 
\begin{equation}
    n^{-1} \sum\limits_{i=1}^n x_i(y_i - \beta_0 - \beta_1x_i) = 0
\end{equation}
también llamada \textbf{condiciones de primer orden para los estimadores de MCO}. Luego por la ecuación (2.9) tenemos que 
\begin{equation}
    \overline{y} = \hat{\beta_0} + \hat{\beta_1}\overline{x}.
\end{equation}
donde $\overline{y} = n^{-1}\sum\limits_{i=1}^n y_i$ es el promedio muestral de las $y_i$, y lo mismo ocurre con $\overline{x}$, así, 

\begin{tcolorbox}[colframe=white]
\begin{equation}
    \hat{\beta_0} = \overline{y} - \hat{\beta_1}\overline{x}.
\end{equation}
\end{tcolorbox}
Por último empleando (2.10) y (2.12) para sustituir $\hat{\beta_0}$ se obtiene,
$$\sum_{i=1}^n x_i\left[y_i-(\overline{y}-\hat{\beta_1}\overline{x})-\hat{\beta_1}x_i\right] = 0,$$
de donde, reordenando, tenemos que
$$\sum_{i=1}^n x_i(y_i-\overline{y}) = \hat{\beta_1}\sum_{i=1}^n x_i(x_i-\overline{x}).$$
en consecuencia por las propiedades de la sumatoria, 
$$\sum_{i=1}^n x_i(x_i-\overline{x}) = \sum_{i=1}^n (x_i-\overline{x})^2 \quad y \quad \sum_{i=1}^n x_i(y_i-\overline{y}) = \sum_{i=1}^n (x_i-\overline{x})(y_i-\overline{y})$$\\
ya que $\sum\limits_{i=1}^n (x_i-\overline{x})(y_i-\overline{y})=\sum\limits_{i=1}^n x_i y_i - \overline{y}\sum\limits_{i=1}^n x_i - \overline{x}\sum\limits_{i=1}^n y_i + \overline{y} \overline{x}\sum\limits_{i=1}^n 1 $, luego ya que $\sum\limits_{i=1}^n x_i = n\overline{x}$ entonces $\sum\limits_{i=1}^n x_iy_i - n\overline{yx} - n\overline{xy} + n\overline{xy} = \sum\limits_{i=1}^n x_i(y_i-\overline{y})$
por lo tanto, 
\begin{tcolorbox}[colframe=white]
\begin{equation}
    \hat{\beta_1} = \frac{\sum\limits_{i=1}^n (x_i-\overline{x})(y_i-\overline{y})}{\sum\limits_{i=1}^n (x_i-\overline{x})^2}.
\end{equation}
\end{tcolorbox}
Ésta ecuación no es nada mas que la covarianza muestral en $x$ e $y$ dividida entre la variación muestral de $x$. Esto tiene sentido porque $\beta_1$ es igual a la covarianza poblacional dividida entre la varianza de $x$ cuando $E(u)=0$ y $Cov(x,u) = 0$. Como consecuencia directa se tiene que si en la muestra $x$ e $y$ están correlacionadas positivamente, entonces $\hat{\beta_1}$ es positiva y contrariamente.\\\\
Para todo $\hat{\beta_0}$ y $\hat{\beta_1}$ se define el valor ajustado para $y$ cuando $x=x_i$ como 
\begin{equation}
	\hat{y_i} = \hat{\beta_0} + \hat{\beta_1}x_i.
\end{equation}
Este es el valor que se predice para $y$ cuando $x=x_i$.\\\\
El \textbf{residual} de la observación $i$ es la diferencia entre el verdadero valor $y_i$ y su valor ajustado.\begin{equation} 
    \hat{u} = y_i - \hat{y_i} = y_i - \hat{\beta_0} - \hat{\beta_1}x_i.
\end{equation}
Los residuales no son lo mismo que la ecuación (2.2)\\\\
Supongamos que $\hat{\beta_0}$ y $\hat{\beta_1}$, se eligen de manera que la suma de residuales cuadrados, 
\begin{equation}
    \sum\limits_{i=1}^n \hat{u_i}^2 = \sum\limits_{i=1}^n (y_i - \hat{\beta_0} - \hat{\beta_1}x_i)^2
\end{equation}
sea tan pequeña como sea posible. \\\\

\textbf{Minimización de la suma de los residuos cuadrados.-}
Se mostrará que $\hat{\beta_0}$ y $\hat{\beta_1}$ estimados de MCO minimizan la suma de los residuales cuadrados. Formalmente, el problema es encontrar las soluciones $\hat{\beta_0}$ y $\hat{\beta_1}$ del problema de minimización 
$$\min_{b_0,b_1} \sum_{i=1}^n (y_i-b_0-b_ix_i)^2$$
donde $b_0$ y $b_1$ son argumentos ficticios en el problema de optimización. Para simplificar llámesele a esta función $Q(b_0,b_1)$. Una condición para que $\hat{\beta_0}$ y $\hat{\beta_1}$ sean soluciones del problema de minimización es que las derivadas parciales de $Q(b_0,b_1)$ respecto a $b_0$ y $b_1$ evaluadas en $$\hat{\beta_0},\hat{\beta_1}: \quad \dfrac{\partial Q(\hat{\beta_0},\hat{\beta_1})}{\partial b_0}=0 \qquad y \qquad \dfrac{\partial Q(\hat{\beta_0},\hat{\beta_1})}{\partial b_1}=0$$
Con ayuda de la regla de la cadena del cálculo, estas dos ecuaciones se convierten en,
$$-2\sum_{i=1}^n \hat{u} = -2\sum_{i=1}^n(y_i-\hat{\beta}_0 - \hat{\beta}_1 x_i)=0, \qquad -2\sum_{i=1}^n x_1\hat{u} = -2\sum_{i=1}^n x_i(y_i-\hat{\beta}_0 - \hat{\beta}_1 x_i)=0$$
Una manera de comprobar que se ha minimizado la suma de los residuos cuadrados es expresando, para cualquier $b_0$ y $b_1$,
$$\begin{array}{rcl}
    Q(b_0,b_1)&=&\sum\limits_{i=1}^n \left[y_i-\hat{\beta}_0 - \hat{\beta}_1 + (\hat{\beta}_0 - \hat{\beta}_1 - b_0) + (\hat{\beta}_i - b_1)x_i\right]^2\\\\
	      &=&\sum\limits_{i=1}^n \left[\hat{u}_i + (\hat{\beta}_0 - b_0) + (\hat{\beta}_i - b_1)x_i\right]^2\\\\
	      &=&\sum\limits_{i=1}^n \hat{u}_1^2 + n(\hat{\beta}_0-b_0)^2 + (\hat{\beta}_1-b_1)\sum\limits_{i=1}^n x_i^2 + 2(\hat{\beta}_0-b_0)(\hat{\beta}_1-b_1)\sum\limits_{i=1}^n x_i\\\\
\end{array}$$
Luego ya que el primer termino no depende de $b_0$ ni de $b_1$ entonces,
$$\sum_{i=1}^n \left[(\hat{\beta}_0-b_0)+(\hat{\beta}_1-b_1)x_i\right]^2$$
Dado que ésta es una suma de términos al cuadrado, el menor valor que puede tener es 0. Por lo tanto, tendrá el menor valor posible cuando $b_o=\hat{\beta}_0$ y $b_1=\hat{\beta}_1$.\\\\

Con MCO se podrá obtener insesgamiento, consistencia y otras propiedades estadísticas de una manera relativamente sencilla. Además, como sugiere la motivación en las ecuaciones (2.8) y (2.9)  y como se verá en la sección 2.5, el método de MCO es adecuado para estimar los parámetros que aparecen en la función de la media condicional.\\\\

Una vez que se han determinado las estimaciones por MCO del intercepto y de la pendiente,
se obtiene la \textbf{línea de regresión de MCO o función de regresión muestral (FRM)}:

\begin{tcolorbox}[colframe=white]
    \begin{equation}
	\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x.
    \end{equation}
\end{tcolorbox}

Donde se entiende que $\hat{\beta}_0$ y $\hat{\beta}_1$ han sido obtenidas empleando las ecuaciones (2.12) y (2.13). La notación $\hat{y}$ que se lee $y \; gorro$ indica los valores predichos por la ecuación (2.17) son estimaciones. \\\\
En la mayoría de los casos, la pendiente estimada, se puede expresar como,
\begin{equation}
    \hat{\beta}_1 = \dfrac{\triangle \hat{y}}{\triangle x}
\end{equation}
es de mayor interés, pues indica la cantidad en la que cambia $\hat{y}$ cuando $x$ se incrementa en una unidad. De manera equivalente,
\begin{equation}
    \hat{y} =  \hat{\beta}_1 \triangle x.
\end{equation}

\section{Propiedades de MCO en cualquier muestra de datos}
El residual de MCO correspondiente a la observación $i$, $\hat{u}_i$, es la diferencia entre $y_i$ y su valor ajustado, como se indica en la ecuación (2.15). Si $\hat{u}_i$ es positivo, la línea predice un valor inferior al de $y_i$; si $\hat{u}_i$ es negativo, la linea predice un valor superior al de $y_i$. Lo ideal para la observación $i$ es cuando $\hat{u}_i = 0$, pero en la mayoria de los casos todos los residuales son distintos de cero. 

\subsection{Propiedades algebraicas de los estadísticos de MCO}
Las estimaciones de $MCO$ y sus correspondientes estadísticos tiene varias propiedades útiles. A continuación se verán las tres más importantes.
\begin{enumerate}[\bfseries 1.]

    \item La suma y por tanto el promedio muestral de los residuales de MCO, es cero. Matemáticamente
	\begin{equation}
	    \sum_{i=1}^n \hat{u}_i = 0.
	\end{equation}

    \item La covarianza muestral entre los regresores y los residuales de MCO es cero. Esto es consecuencia de la condición de primer orden (2.10) que en términos de los residuales puede expresarse como
    	\begin{equation}
	    \sum_{i=1}^n x_i \hat{u}_i = 0.
	\end{equation}

	El promedio muestral de los residuos de MCO, es cero, por lo que el lado izquierdo de la ecuación anterior es proporcional a la covarianza entre las $x_i$ y los $\hat{u}_i$.

    \item El punto $(\overline{x},\overline{y})$ se encuentran siempre sobre la línea de regresión de MCO. En otras palabras, si en la ecuación $\hat{y} = \hat{beta}_i x + \hat{beta}_o$ se sustituye $\overline{x}$ por $x$, el valor predicho es $\overline{y}$. Esto es exactamente lo que dice la ecuación $\overline{y} = \hat{\beta}_0 + \hat{beta}_1 \overline{x}$.\\\\
\end{enumerate}

