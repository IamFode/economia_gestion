\chapter{Los elementos tecnológicos de la inteligencia artificial}

\section{Introducción}
En este capítulo, definiremos un marco para pensar en los ingredientes de esta nueva IA impulsada por ML. Tener una comprensión de las piezas que componen estos sistemas y cómo encajan entre sí es importante para aquellos que construirán negocios en torno a esta tecnología.

\section{Que es IA}
Necesita un banco masivo de datos para poner el sistema en funcionamiento y una estrategia para continuar generando datos para que el sistema pueda responder y aprender. Y finalmente, necesita rutinas de aprendizaje automático que puedan detectar patrones y hacer predicciones a partir de los datos no estructurados.\\
El aprendizaje automático puede hacer cosas fantásticas, pero básicamente se limita a predecir un futuro que se parece principalmente al pasado. Estas son herramientas para el reconocimiento de patrones. Por el contrario, un sistema de IA puede resolver problemas complejos que antes estaban reservados para los humanos.\\
La inteligencia artificial utiliza instancias de aprendizaje automático como componentes del sistema más grande. Estas instancias de ML deben organizarse dentro de una estructura definida por el conocimiento del dominio, y deben recibir datos que les ayuden a completar las tareas de predicción asignadas.\\
La evolución de ML hacia el estatus de tecnología de propósito general es el principal impulsor del actual auge de la IA. Sin embargo, los algoritmos de ML son componentes básicos de la IA dentro de un contexto más amplio. El punto clave aquí es que, si bien las tareas de ML compuestas pueden atacarse con DNN relativamente genéricos, el sistema combinado completo se construye de una manera altamente especializada para la estructura del problema en cuestión.\\
Para lidiar con el mundo real, debe tener una teoría sobre las reglas del juego relevante. Por ejemplo, si desea crear un sistema que pueda comunicarse con los clientes, puede proceder mapeando los deseos e intenciones de los clientes de tal manera que permita diferentes rutinas de aprendizaje automático que generen diálogos para cada uno. O bien, para cualquier sistema de IA que se ocupe del marketing y los precios en un entorno minorista, debe poder utilizar la estructura de un sistema de demanda económica para pronosticar cómo cambiará el precio de un solo artículo (lo que podría, digamos, ser el trabajo). De un solo DNN) afectará los precios óptimos para otros productos y el comportamiento de sus consumidores (quienes podrían ser modelados con DNN).\\
Como detallaremos a continuación, el aprendizaje automático en su forma actual se ha convertido en una tecnología de propósito general (Bresnahan 2010). Estas herramientas se volverán más baratas y rápidas con el tiempo, debido a las innovaciones en el propio ML. Quienes tengan la experiencia que pueda descomponer complejos problemas empresariales humanos en componentes que se puedan resolver con ML tendrán éxito en la construcción de la próxima generación de inteligencia artificial empresarial, que puede hacer más que solo jugar juegos.\\
En muchos de estos escenarios, las ciencias sociales tendrán un papel que desempeñar. La ciencia se trata de poner estructura y teoría en torno a fenómenos que son increíblemente complejos desde el punto de vista de la observación. A menudo se confiará en la economía, como la ciencia social más cercana a los negocios, para proporcionar las reglas para la IA empresarial. Y dado que la IA impulsada por ML se basa en la medición de recompensas y parámetros dentro de su contexto, la econometría desempeñará un papel clave en el puente entre el sistema supuesto y las señales de datos utilizadas para la retroalimentación y el aprendizaje. La obra no se traducirá directamente. Necesitamos construir sistemas que permitan un cierto margen de error en los algoritmos de ML. Esas teorías económicas que se aplican solo a un conjunto muy limitado de condiciones, por ejemplo, en el equilibrio del filo de una navaja, serán demasiado inestables para la IA. Hay un futuro emocionante aquí donde los economistas pueden contribuir a la ingeniería de IA, y tanto la IA como la economía avanzan a medida que aprendemos qué recetas funcionan o no para la IA comercial.\\
Más allá del aprendizaje automático y la estructura del dominio, el tercer pilar de la IA es la generación de datos. Estoy usando el término generación aquí, en lugar de un término más pasivo como colección, para resaltar que los sistemas de IA requieren una estrategia activa para mantener un flujo constante de información nueva y útil que fluye hacia los algoritmos de aprendizaje compuestos. En la mayoría de las aplicaciones de IA habrá dos clases generales de datos.\\
El marco general de los algoritmos de ML que eligen activamente los datos que consumen se conoce como aprendizaje reforzado (RL). Es un aspecto muy importante de la IA impulsada por ML. En algunos escenarios estrechos y altamente estructurados, los investigadores han creado sistemas de aprendizaje de disparo cero en los que la IA puede lograr un alto rendimiento después de comenzar sin ningún dato de entrenamiento estático.\\
Como complemento al trabajo sobre el aprendizaje por refuerzo, hay mucha actividad de investigación en torno a los sistemas de IA que pueden simular datos para que parezcan que provienen de una fuente del mundo real. Esto tiene el potencial de acelerar el entrenamiento del sistema, replicando el éxito que ha tenido el campo con los videojuegos y los juegos de mesa donde la experimentación es prácticamente gratuita (simplemente juegue, nadie pierde dinero ni se lastima). Las redes antagónicas generativas (GAN; Goodfellow et al. 2014) son esquemas en los que una DNN simula datos y otra intenta discernir qué datos son reales y cuáles son simulados.

\section{Machine Learning de uso general}
La pieza de IA que recibe más publicidad, tanto que a menudo se confunde con toda la IA, es el aprendizaje automático de propósito general. Independientemente de este ligero énfasis excesivo, está claro que el reciente auge de las redes neuronales profundas (DNN) es un motor principal detrás del crecimiento de la IA. Estos DNN tienen la capacidad de aprender patrones en datos de voz, imagen y video (así como en datos estructurados más tradicionales) más rápido y más automáticamente que nunca.\\
El aprendizaje automático es el campo que piensa en cómo construir automáticamente predicciones sólidas a partir de datos complejos. Está estrechamente relacionado con las estadísticas modernas y, de hecho, muchas de las mejores ideas en ML provienen de estadísticos (el lazo, los árboles, los bosques, etc.). Pero mientras que los estadísticos a menudo se han centrado en la inferencia de modelos, en la comprensión de los parámetros de sus modelos (por ejemplo, probar los coeficientes individuales en una regresión), la comunidad de ML se ha centrado más en el objetivo único de maximizar el rendimiento predictivo. Todo el campo de ML se calibra contra experimentos fuera de la muestra que evalúan qué tan bien un modelo entrenado en un conjunto de datos predecirá nuevos datos.\\
La predicción es más fácil que la inferencia del modelo. Esto ha permitido a la comunidad de ML avanzar rápidamente y trabajar con datos más grandes y complejos. También facilitó un enfoque en la automatización: desarrollar algoritmos que funcionarán en una variedad de diferentes tipos de datos con poco o ningún ajuste requerido. Hemos visto una explosión de herramientas de ML de propósito general en la última década, herramientas que pueden implementarse en datos desordenados y ajustarse automáticamente para un rendimiento predictivo óptimo.\\
Una de las áreas más activas actualmente de la ciencia de datos es la combinación de herramientas de ML con el tipo de inferencia contrafactual que los econometristas han estudiado durante mucho tiempo, por lo que ahora se fusiona el material de ML y estadísticas con el trabajo de los economistas. Véase, por ejemplo, Athey e Imbens (2016), Hartford et al. (2017), y la encuesta en Athey (2017).\\
La última generación de algoritmos ML, especialmente la tecnología de aprendizaje profundo que se ha disparado desde alrededor de 2012 (Krizhevsky, Sutskever y Hinton 2012), ha aumentado el nivel de automatización en el proceso de ajuste y aplicación de modelos de predicción. Esta nueva clase de ML es el ML de propósito general (GPML).\\
El primer componente de GPML son las redes neuronales profundas(deep neural network): modelos formados por capas de funciones de nodo de transformación no lineal, donde la salida de cada capa se convierte en entrada para la siguiente capa de la red que se hacen que sea más rápido y fácil que nunca encontrar patrones en datos no estructurados. También son altamente modulares. Puede tomar una capa que esté optimizada para un tipo de datos ( ej., imágenes) y combinarla con otras capas para otros tipos de datos (ej., texto). También puede usar capas que se han entrenado previamente en un conjunto de datos (ej., imágenes genéricas) como componentes en un modelo más especializado (ej., una tarea de reconocimiento específica).\\
Las arquitecturas DNN especializadas son responsables de la capacidad clave de GPML de trabajar con datos a nivel humano: video, audio y texto. Esto es esencial para la IA porque permite que estos sistemas se instalen sobre las mismas fuentes de conocimiento que los humanos pueden digerir. No necesita crear un nuevo sistema de base de datos (o tener un formulario estándar existente) para alimentar la IA; más bien, la IA puede vivir por encima del caos de información generado a través de las funciones comerciales. Esta capacidad ayuda a ilustrar por qué la nueva IA, basada en GPML, es mucho más prometedora que los intentos anteriores de IA. La IA clásica se basaba en reglas lógicas especificadas a mano para imitar cómo un ser humano racional podría abordar un problema determinado (Haugeland 1985).\\
Las herramientas que facilitan el ajuste de modelos en conjuntos de datos masivos son: validación fuera de muestra (OOS) para ajuste de modelos, descenso de gradiente estocástico (SGD) para optimización de parámetros , y unidades de procesamiento gráfico (GPU) y otro hardware informático para la optimización paralela masiva. Cada una de estas piezas es esencial para el éxito de GPML a gran escala.\\
Una de las características distintivas de una tecnología de propósito general es que conduce a amplios cambios industriales, tanto por encima como por debajo de donde vive esa tecnología en la cadena de suministro.

\section{Deep Learning}
un Buen libro de referencia para guia de usuario es la de Goodfellow, bengio y courville (2016).\\
El entusiasmo en esta área, y la considerable exageración de los medios y los negocios, hace que sea difícil hacer un seguimiento. Además, la tendencia de las empresas y los académicos de ML de proclamar cada cambio incremental como "completamente nuevo" ha llevado a una literatura desordenada que es difícil de navegar para los recién llegados. Pero hay una estructura general para el aprendizaje profundo, y una comprensión libre de exageraciones de esta estructura debería darle una idea de las razones del éxito de esto.\\
Las redes neuronales son modelos simples. De hecho, su simplicidad es una fortaleza: los patrones básicos facilitan el entrenamiento y el cálculo rápidos. El modelo tiene combinaciones lineales de entradas que pasan a través de funciones de activación no lineales llamadas nodos (o, en referencia al cerebro humano, neuronas). Un conjunto de nodos que toman diferentes sumas ponderadas de las mismas entradas se denomina capa, y la salida de los nodos de una capa se convierte en la entrada de la siguiente capa.\\
Las redes neuronales tienen una larga historia. El trabajo en este tipo de modelos se remonta a mediados del siglo XX, por ejemplo, incluido el Perceptron de Rosenblatt (Rosenblatt 1958). Este trabajo inicial se centró en las redes como modelos que podrían imitar la estructura real del cerebro humano. A fines de la década de 1980, los avances en algoritmos para entrenar redes neuronales (Rumelhart et al. 1988) abrieron el potencial para que estos modelos actuaran como herramientas generales de reconocimiento de patrones en lugar de un modelo de juguete del cerebro. Esto condujo a un auge en la investigación de redes neuronales, y los métodos desarrollados durante la década de 1990 son la base de gran parte del aprendizaje profundo actual (Hochreiter y Schmidhuber 1997; LeCun et al. 1998). Sin embargo, este auge terminó en quiebra. Debido a la brecha entre los resultados prometidos y los realizados (y las dificultades persistentes en el entrenamiento de redes en conjuntos de datos masivos) desde fines de la década de 1990, las redes neuronales se convirtieron en solo un método de ML entre muchos. En las aplicaciones, fueron suplantados por herramientas más sólidas como Random Forests, regresión regularizada de alta dimensión y una variedad de modelos de procesos estocásticos bayesianos.\\
cerca del final de la década de 1990, Radford Neal demostró que ciertas redes neuronales convergen hacia los procesos gaussianos, un modelo de regresión estadística clásico, a medida que el número de nodos en una sola capa crece hasta el infinito (Neal 2012). Parecía razonable concluir que las redes neuronales eran solo versiones toscas de modelos estadísticos más transparentes. ¿Qué cambió? Un montón de cosas. Dos eventos no metodológicos son de importancia primordial: obtuvimos muchos más datos (big data) y el hardware informático se volvió mucho más eficiente (GPU). Pero también hubo un desarrollo metodológico crucial: las redes fueron profundas. Este avance a menudo se atribuye al trabajo de 2006 de Geoff Hinton y sus coautores (Hinton, Osindero y Teh 2006) en una arquitectura de red que apiló muchas capas previamente entrenadas para una tarea de reconocimiento de escritura a mano. En este preentrenamiento, las capas interiores de la red se ajustan mediante una tarea de aprendizaje no supervisado (es decir, la reducción de la dimensión de las entradas) antes de ser utilizadas como parte de la maquinaria de aprendizaje supervisado. La idea es análoga a la de la regresión de componentes principales: primero ajusta una representación de baja dimensión de x, luego usa esa representación de baja D para predecir alguna y asociada. El esquema de Hinton y sus colegas permitió a los investigadores entrenar redes más profundas de lo que antes era posible.\\
Este tipo específico de preentrenamiento no supervisado ya no se considera central para el aprendizaje profundo. Sin embargo, el artículo de Hinton, Osindero y Teh (2006) abrió los ojos de muchas personas al potencial de las redes neuronales profundas: modelos con muchas capas, cada una de las cuales puede tener una estructura diferente y desempeñar un papel muy diferente en la maquinaria general. Es decir, una demostración de que uno podía entrenar redes profundas pronto se convirtió en la comprensión de que uno debería agregar profundidad a los modelos. En los años siguientes, los grupos de investigación comenzaron a mostrar empírica y teóricamente que la profundidad era importante para aprender eficientemente de los datos (Bengio et al. 2007). La modularidad de una red profunda es clave: cada capa de la estructura funcional desempeña un papel específico, y puede intercambiar capas como bloques de Lego cuando se mueve entre aplicaciones de datos. Esto permite un rápido desarrollo de modelos específicos de la aplicación y también la transferencia del aprendizaje entre modelos: una capa interna de una red que ha sido entrenada para un tipo de problema de reconocimiento de imágenes se puede usar para iniciar una nueva red para un problema diferente. Tarea de visión artificial.\\
El aprendizaje profundo entró en la corriente principal de ML con un artículo de 2012 de Krizhevsky, Sutskever y Hinton (2012) que mostró que su DNN podía superar los puntos de referencia de rendimiento actuales en el conocido concurso de visión por computadora ImageNet. Por ejemplo, las DNN ahora pueden reconocer imágenes y generar subtítulos apropiados (Karpathy y Fei-Fei 2015).\\
Las llamadas redes neuronales convolucionales (CNN; LeCun y Bengio 1995) ilustran la estrategia que hace que el aprendizaje profundo sea tan exitoso: es conveniente apilar capas de diferentes especializaciones de modo que las funciones específicas de la imagen (convoluciones) puedan alimentar capas que son bueno para representar formas funcionales genéricas. Por ejemplo, la arquitectura muy simple que usamos en Hartford et al. (2017) para una tarea que mezclaba reconocimiento de dígitos con datos comerciales (simulados).\\
Para datos de texto, debe incrustar palabras en un espacio vectorial. Esto puede suceder a través de una simple transformación word2vec (Mikolov et al. 2013) (una descomposición lineal en la matriz de conteos de co-ocurrencia para palabras; por ejemplo, dentro de tres palabras entre sí) o a través de una arquitectura LSTM (memoria a largo plazo) (Hochreiter y Schmidhuber 1997): modelos para secuencias de palabras o letras que esencialmente mezclan un modelo oculto de Markov (largo) con un proceso autorregresivo (corto). Y hay muchas otras variantes, con nuevas arquitecturas que se desarrollan todos los días.\\
Una cosa debe quedar clara: hay mucha estructura en los DNN. Estos modelos no son similares a los tipos de modelos de regresión no paramétricos utilizados por estadísticos, econometristas y en ML anterior. Son semiparamétricos. Por lo tanto, las DNN combinan la reducción de dimensiones restrictivas con la aproximación de funciones flexibles. La clave es que ambos componentes se aprenden de forma conjunta.\\
Como advertimos al principio, hemos cubierto solo una pequeña parte del área de aprendizaje profundo. Hay un montón de material nuevo y emocionante que sale tanto de la industria como de la academia. (Para echar un vistazo a lo que está sucediendo en el campo, explore las últimas actas de NIPS [Sistemas de procesamiento de información neuronal, la principal conferencia de ML] https://papers.nips.cc/.\\
Los temas de este tipo están cobrando importancia a medida que las DNN se alejan de las competencias académicas y se acercan a las aplicaciones del mundo real. A medida que el campo crece y la construcción de modelos de DNN pasa de una disciplina científica a una de ingeniería, veremos una mayor necesidad de este tipo de investigación que nos dice cuándo y cuánto podemos confiar en las DNN.


\section{Stochastic Gradient Descent}
Para brindar una visión completa del aprendizaje profundo, debemos describir el único algoritmo en el que se confía para entrenar todos los modelos: SGD. La optimización del descenso de gradiente estocástico es un giro en el descenso de gradiente (GD), el método dominante anteriormente para minimizar cualquier función que pueda diferenciar.\\
El descenso de gradiente GD es la mejor herramienta de optimización que tenemos, pero se vuelve computacionalmente inviable para conjuntos de datos masivos. La solución es reemplazar los gradientes reales con estimaciones de esos gradientes basados en un subconjunto de datos. Este es el algoritmo SGD. Tiene una larga historia, que se remonta al algoritmo Robbins-Monro (Robbins y Monro 1951) propuesto por un par de estadísticos en 1951. En las versiones más comunes de SGD, el gradiente de muestra completa simplemente se reemplaza por el gradiente en un pequeña submuestra.\\
Para comprender por qué SGD es tan preferible a GD para el aprendizaje automático, es útil analizar cómo piensan los informáticos sobre las restricciones en la estimación. Los estadísticos y los economistas tienden a ver el tamaño de la muestra (es decir, la falta de datos) como la restricción vinculante para sus estimadores. Por el contrario, en muchas aplicaciones de ML, los datos son prácticamente ilimitados y continúan creciendo durante la implementación del sistema. A pesar de esta abundancia, existe un presupuesto computacional fijo (o la necesidad de actualizar casi en tiempo real para la transmisión de datos), de modo que solo podemos ejecutar una cantidad limitada de operaciones cuando analizamos los datos. Por lo tanto, en ML, la restricción vinculante es la cantidad de cómputo en lugar de la cantidad de datos.\\
El descenso de gradiente estocástico intercambia actualizaciones más rápidas por una tasa de convergencia por actualización más lenta. Como se explica muy bien en un artículo de 2008 de Bottou y Bousquet (Bottou y Bousquet 2008), este intercambio vale la pena cuando las actualizaciones más rápidas le permiten exponer su modelo a más datos de lo que sería posible de otro modo.\\
La confianza en SGD ha hecho que sea relativamente fácil para el aprendizaje profundo progresar de una disciplina científica a una de ingeniería. Cuanto más rápido, mejor, por lo que los ingenieros que ajustan los algoritmos SGD para DNN pueden centrarse simplemente en la velocidad de convergencia.\\

\section{Reinforcement Learning}
Como nuestra sección final sobre los elementos del aprendizaje profundo, consideraremos cómo estos sistemas de IA generan sus propios datos de entrenamiento a través de una combinación de experimentación y optimización. El aprendizaje por refuerzo (RL) es el término común para este aspecto de la IA. El aprendizaje por refuerzo a veces se usa para denotar algoritmos específicos, pero lo estamos usando para referirnos al área completa de recopilación activa de datos.\\

\section{IA en contexto}
Este capítulo ha proporcionado una introducción a los ingredientes clave de la IA. También hemos estado presionando algunos puntos generales. Primero, la ola actual de IA impulsada por ML debe verse como una nueva clase de productos que crecen en torno a una nueva tecnología de propósito general: aprendizaje automático a gran escala, rápido y sólido. La inteligencia artificial no es aprendizaje automático, pero el ML de propósito general, específicamente el aprendizaje profundo, es el motor eléctrico de la IA. Estas herramientas de ML seguirán mejorando, más rápido y más barato. Los recursos de hardware y big data se están adaptando a las demandas de las DNN, y las soluciones de ML de autoservicio están disponibles en todas las principales plataformas de computación en la nube. Las DNN capacitadas podrían convertirse en un producto básico en el futuro cercano, y el mercado de aprendizaje profundo podría verse envuelto en la batalla más grande por la participación de mercado en los servicios de computación en la nube.\\
En segundo lugar, todavía estamos esperando soluciones de IA empresarial verdaderamente integrales que impulsen un aumento real de la productividad. Las ganancias actuales de la IA se limitan principalmente a entornos con una gran cantidad de estructura explícita, como juegos de mesa y videojuegos. Esto está cambiando, ya que empresas como Microsoft y Amazon producen sistemas semiautónomos que pueden abordar problemas comerciales reales. Pero aún queda mucho trabajo por hacer, y los avances los harán quienes puedan imponer estructura a estos complejos problemas empresariales. Es decir, para que la IA empresarial tenga éxito, necesitamos combinar GPML y big data con personas que conozcan las reglas del juego en su dominio empresarial.\\
Finalmente, todo esto tendrá importantes implicaciones para el papel de la economía en la industria. En muchos casos, los economistas son los que pueden proporcionar estructura y reglas en torno a escenarios comerciales complicados. Por ejemplo, un buen econometrista estructural (McFadden 1980; Heckman 1977; Deaton y Muellbauer 1980) utiliza la teoría económica para descomponer una pregunta sustantiva en un conjunto de ecuaciones medibles (es decir, identificadas) con parámetros que pueden estimarse a partir de datos. En muchos entornos, este es exactamente el tipo de flujo de trabajo requerido para la IA. La diferencia es que, en lugar de limitarse a la regresión lineal básica, estas piezas medibles del sistema serán DNN que pueden experimentar activamente y generar sus propios datos de entrenamiento. La próxima generación de economistas debe sentirse cómoda sabiendo cómo aplicar la teoría económica para obtener dicha estructura y cómo traducir esta estructura en recetas que puedan automatizarse con ML y RL. Así como el big data condujo a la ciencia de datos, una nueva disciplina que combina estadística e informática, la IA requerirá pioneros interdisciplinarios que puedan combinar economía, estadística y aprendizaje automático.\\
\\
Finalmente, todo esto tendrá importantes implicaciones para el papel de la economía en la industria. En muchos casos, los economistas son los que pueden proporcionar estructura y reglas en torno a escenarios comerciales complicados. Por ejemplo, un buen econometrista estructural (McFadden 1980; Heckman 1977; Deaton y Muellbauer 1980) utiliza la teoría económica para descomponer una pregunta sustantiva en un conjunto de ecuaciones medibles (es decir, identificadas) con parámetros que pueden estimarse a partir de datos. En muchos entornos, este es exactamente el tipo de flujo de trabajo requerido para la IA. La diferencia es que, en lugar de limitarse a la regresión lineal básica, estas piezas medibles del sistema serán DNN que pueden experimentar activamente y generar sus propios datos de entrenamiento. La próxima generación de economistas debe sentirse cómoda sabiendo cómo aplicar la teoría económica para obtener dicha estructura y cómo traducir esta estructura en recetas que puedan automatizarse con ML y RL. Así como el big data condujo a la ciencia de datos, una nueva disciplina que combina estadística e informática, la IA requerirá pioneros interdisciplinarios que puedan combinar economía, estadística y aprendizaje automático.\\


