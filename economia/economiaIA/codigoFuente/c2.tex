\chapter{Los elementos tecnológicos de la inteligencia artificial}

\section{Introducción}
En este capítulo, definiremos un marco para pensar en los ingredientes de esta nueva IA impulsada por ML. Tener una comprensión de las piezas que componen estos sistemas y cómo encajan entre sí es importante para aquellos que construirán negocios en torno a esta tecnología.

\section{Que es IA}
Necesita un banco masivo de datos para poner el sistema en funcionamiento y una estrategia para continuar generando datos para que el sistema pueda responder y aprender. Y finalmente, necesita rutinas de aprendizaje automático que puedan detectar patrones y hacer predicciones a partir de los datos no estructurados.\\
El aprendizaje automático puede hacer cosas fantásticas, pero básicamente se limita a predecir un futuro que se parece principalmente al pasado. Estas son herramientas para el reconocimiento de patrones. Por el contrario, un sistema de IA puede resolver problemas complejos que antes estaban reservados para los humanos.\\
La inteligencia artificial utiliza instancias de aprendizaje automático como componentes del sistema más grande. Estas instancias de ML deben organizarse dentro de una estructura definida por el conocimiento del dominio, y deben recibir datos que les ayuden a completar las tareas de predicción asignadas.\\
La evolución de ML hacia el estatus de tecnología de propósito general es el principal impulsor del actual auge de la IA. Sin embargo, los algoritmos de ML son componentes básicos de la IA dentro de un contexto más amplio. El punto clave aquí es que, si bien las tareas de ML compuestas pueden atacarse con DNN relativamente genéricos, el sistema combinado completo se construye de una manera altamente especializada para la estructura del problema en cuestión.\\
Para lidiar con el mundo real, debe tener una teoría sobre las reglas del juego relevante. Por ejemplo, si desea crear un sistema que pueda comunicarse con los clientes, puede proceder mapeando los deseos e intenciones de los clientes de tal manera que permita diferentes rutinas de aprendizaje automático que generen diálogos para cada uno. O bien, para cualquier sistema de IA que se ocupe del marketing y los precios en un entorno minorista, debe poder utilizar la estructura de un sistema de demanda económica para pronosticar cómo cambiará el precio de un solo artículo (lo que podría, digamos, ser el trabajo). De un solo DNN) afectará los precios óptimos para otros productos y el comportamiento de sus consumidores (quienes podrían ser modelados con DNN).\\
Como detallaremos a continuación, el aprendizaje automático en su forma actual se ha convertido en una tecnología de propósito general (Bresnahan 2010). Estas herramientas se volverán más baratas y rápidas con el tiempo, debido a las innovaciones en el propio ML. Quienes tengan la experiencia que pueda descomponer complejos problemas empresariales humanos en componentes que se puedan resolver con ML tendrán éxito en la construcción de la próxima generación de inteligencia artificial empresarial, que puede hacer más que solo jugar juegos.\\
En muchos de estos escenarios, las ciencias sociales tendrán un papel que desempeñar. La ciencia se trata de poner estructura y teoría en torno a fenómenos que son increíblemente complejos desde el punto de vista de la observación. A menudo se confiará en la economía, como la ciencia social más cercana a los negocios, para proporcionar las reglas para la IA empresarial. Y dado que la IA impulsada por ML se basa en la medición de recompensas y parámetros dentro de su contexto, la econometría desempeñará un papel clave en el puente entre el sistema supuesto y las señales de datos utilizadas para la retroalimentación y el aprendizaje. La obra no se traducirá directamente. Necesitamos construir sistemas que permitan un cierto margen de error en los algoritmos de ML. Esas teorías económicas que se aplican solo a un conjunto muy limitado de condiciones, por ejemplo, en el equilibrio del filo de una navaja, serán demasiado inestables para la IA. Hay un futuro emocionante aquí donde los economistas pueden contribuir a la ingeniería de IA, y tanto la IA como la economía avanzan a medida que aprendemos qué recetas funcionan o no para la IA comercial.\\
Más allá del aprendizaje automático y la estructura del dominio, el tercer pilar de la IA es la generación de datos. Estoy usando el término generación aquí, en lugar de un término más pasivo como colección, para resaltar que los sistemas de IA requieren una estrategia activa para mantener un flujo constante de información nueva y útil que fluye hacia los algoritmos de aprendizaje compuestos. En la mayoría de las aplicaciones de IA habrá dos clases generales de datos.\\
El marco general de los algoritmos de ML que eligen activamente los datos que consumen se conoce como aprendizaje reforzado (RL). Es un aspecto muy importante de la IA impulsada por ML. En algunos escenarios estrechos y altamente estructurados, los investigadores han creado sistemas de aprendizaje de disparo cero en los que la IA puede lograr un alto rendimiento después de comenzar sin ningún dato de entrenamiento estático.\\
Como complemento al trabajo sobre el aprendizaje por refuerzo, hay mucha actividad de investigación en torno a los sistemas de IA que pueden simular datos para que parezcan que provienen de una fuente del mundo real. Esto tiene el potencial de acelerar el entrenamiento del sistema, replicando el éxito que ha tenido el campo con los videojuegos y los juegos de mesa donde la experimentación es prácticamente gratuita (simplemente juegue, nadie pierde dinero ni se lastima). Las redes antagónicas generativas (GAN; Goodfellow et al. 2014) son esquemas en los que una DNN simula datos y otra intenta discernir qué datos son reales y cuáles son simulados.

\section{Machine Learning de uso general}
La pieza de IA que recibe más publicidad, tanto que a menudo se confunde con toda la IA, es el aprendizaje automático de propósito general. Independientemente de este ligero énfasis excesivo, está claro que el reciente auge de las redes neuronales profundas (DNN) es un motor principal detrás del crecimiento de la IA. Estos DNN tienen la capacidad de aprender patrones en datos de voz, imagen y video (así como en datos estructurados más tradicionales) más rápido y más automáticamente que nunca.\\
El aprendizaje automático es el campo que piensa en cómo construir automáticamente predicciones sólidas a partir de datos complejos. Está estrechamente relacionado con las estadísticas modernas y, de hecho, muchas de las mejores ideas en ML provienen de estadísticos (el lazo, los árboles, los bosques, etc.). Pero mientras que los estadísticos a menudo se han centrado en la inferencia de modelos, en la comprensión de los parámetros de sus modelos (por ejemplo, probar los coeficientes individuales en una regresión), la comunidad de ML se ha centrado más en el objetivo único de maximizar el rendimiento predictivo. Todo el campo de ML se calibra contra experimentos fuera de la muestra que evalúan qué tan bien un modelo entrenado en un conjunto de datos predecirá nuevos datos.\\
La predicción es más fácil que la inferencia del modelo. Esto ha permitido a la comunidad de ML avanzar rápidamente y trabajar con datos más grandes y complejos. También facilitó un enfoque en la automatización: desarrollar algoritmos que funcionarán en una variedad de diferentes tipos de datos con poco o ningún ajuste requerido. Hemos visto una explosión de herramientas de ML de propósito general en la última década, herramientas que pueden implementarse en datos desordenados y ajustarse automáticamente para un rendimiento predictivo óptimo.\\
Una de las áreas más activas actualmente de la ciencia de datos es la combinación de herramientas de ML con el tipo de inferencia contrafactual que los econometristas han estudiado durante mucho tiempo, por lo que ahora se fusiona el material de ML y estadísticas con el trabajo de los economistas. Véase, por ejemplo, Athey e Imbens (2016), Hartford et al. (2017), y la encuesta en Athey (2017).\\
La última generación de algoritmos ML, especialmente la tecnología de aprendizaje profundo que se ha disparado desde alrededor de 2012 (Krizhevsky, Sutskever y Hinton 2012), ha aumentado el nivel de automatización en el proceso de ajuste y aplicación de modelos de predicción. Esta nueva clase de ML es el ML de propósito general (GPML).\\
El primer componente de GPML son las redes neuronales profundas(deep neural network): modelos formados por capas de funciones de nodo de transformación no lineal, donde la salida de cada capa se convierte en entrada para la siguiente capa de la red que se hacen que sea más rápido y fácil que nunca encontrar patrones en datos no estructurados. También son altamente modulares. Puede tomar una capa que esté optimizada para un tipo de datos ( ej., imágenes) y combinarla con otras capas para otros tipos de datos (ej., texto). También puede usar capas que se han entrenado previamente en un conjunto de datos (ej., imágenes genéricas) como componentes en un modelo más especializado (ej., una tarea de reconocimiento específica).\\
Las arquitecturas DNN especializadas son responsables de la capacidad clave de GPML de trabajar con datos a nivel humano: video, audio y texto. Esto es esencial para la IA porque permite que estos sistemas se instalen sobre las mismas fuentes de conocimiento que los humanos pueden digerir. No necesita crear un nuevo sistema de base de datos (o tener un formulario estándar existente) para alimentar la IA; más bien, la IA puede vivir por encima del caos de información generado a través de las funciones comerciales. Esta capacidad ayuda a ilustrar por qué la nueva IA, basada en GPML, es mucho más prometedora que los intentos anteriores de IA. La IA clásica se basaba en reglas lógicas especificadas a mano para imitar cómo un ser humano racional podría abordar un problema determinado (Haugeland 1985).\\
Las herramientas que facilitan el ajuste de modelos en conjuntos de datos masivos son: validación fuera de muestra (OOS) para ajuste de modelos, descenso de gradiente estocástico (SGD) para optimización de parámetros , y unidades de procesamiento gráfico (GPU) y otro hardware informático para la optimización paralela masiva. Cada una de estas piezas es esencial para el éxito de GPML a gran escala.\\
Una de las características distintivas de una tecnología de propósito general es que conduce a amplios cambios industriales, tanto por encima como por debajo de donde vive esa tecnología en la cadena de suministro.

\section{Deep Learning}
un Buen libro de referencia para guia de usuario es la de Goodfellow, bengio y courville (2016).\\
El entusiasmo en esta área, y la considerable exageración de los medios y los negocios, hace que sea difícil hacer un seguimiento. Además, la tendencia de las empresas y los académicos de ML de proclamar cada cambio incremental como "completamente nuevo" ha llevado a una literatura desordenada que es difícil de navegar para los recién llegados. Pero hay una estructura general para el aprendizaje profundo, y una comprensión libre de exageraciones de esta estructura debería darle una idea de las razones del éxito de esto.\\
Las redes neuronales son modelos simples. De hecho, su simplicidad es una fortaleza: los patrones básicos facilitan el entrenamiento y el cálculo rápidos. El modelo tiene combinaciones lineales de entradas que pasan a través de funciones de activación no lineales llamadas nodos (o, en referencia al cerebro humano, neuronas). Un conjunto de nodos que toman diferentes sumas ponderadas de las mismas entradas se denomina capa, y la salida de los nodos de una capa se convierte en la entrada de la siguiente capa.\\
Las redes neuronales tienen una larga historia. El trabajo en este tipo de modelos se remonta a mediados del siglo XX, por ejemplo, incluido el Perceptron de Rosenblatt (Rosenblatt 1958). Este trabajo inicial se centró en las redes como modelos que podrían imitar la estructura real del cerebro humano. A fines de la década de 1980, los avances en algoritmos para entrenar redes neuronales (Rumelhart et al. 1988) abrieron el potencial para que estos modelos actuaran como herramientas generales de reconocimiento de patrones en lugar de un modelo de juguete del cerebro. Esto condujo a un auge en la investigación de redes neuronales, y los métodos desarrollados durante la década de 1990 son la base de gran parte del aprendizaje profundo actual (Hochreiter y Schmidhuber 1997; LeCun et al. 1998). Sin embargo, este auge terminó en quiebra. Debido a la brecha entre los resultados prometidos y los realizados (y las dificultades persistentes en el entrenamiento de redes en conjuntos de datos masivos) desde fines de la década de 1990, las redes neuronales se convirtieron en solo un método de ML entre muchos. En las aplicaciones, fueron suplantados por herramientas más sólidas como Random Forests, regresión regularizada de alta dimensión y una variedad de modelos de procesos estocásticos bayesianos.\\
cerca del final de la década de 1990, Radford Neal demostró que ciertas redes neuronales convergen hacia los procesos gaussianos, un modelo de regresión estadística clásico, a medida que el número de nodos en una sola capa crece hasta el infinito (Neal 2012). Parecía razonable concluir que las redes neuronales eran solo versiones toscas de modelos estadísticos más transparentes. ¿Qué cambió? Un montón de cosas. Dos eventos no metodológicos son de importancia primordial: obtuvimos muchos más datos (big data) y el hardware informático se volvió mucho más eficiente (GPU). Pero también hubo un desarrollo metodológico crucial: las redes fueron profundas. Este avance a menudo se atribuye al trabajo de 2006 de Geoff Hinton y sus coautores (Hinton, Osindero y Teh 2006) en una arquitectura de red que apiló muchas capas previamente entrenadas para una tarea de reconocimiento de escritura a mano. En este preentrenamiento, las capas interiores de la red se ajustan mediante una tarea de aprendizaje no supervisado (es decir, la reducción de la dimensión de las entradas) antes de ser utilizadas como parte de la maquinaria de aprendizaje supervisado. La idea es análoga a la de la regresión de componentes principales: primero ajusta una representación de baja dimensión de x, luego usa esa representación de baja D para predecir alguna y asociada. El esquema de Hinton y sus colegas permitió a los investigadores entrenar redes más profundas de lo que antes era posible.\\
Este tipo específico de preentrenamiento no supervisado ya no se considera central para el aprendizaje profundo. Sin embargo, el artículo de Hinton, Osindero y Teh (2006) abrió los ojos de muchas personas al potencial de las redes neuronales profundas: modelos con muchas capas, cada una de las cuales puede tener una estructura diferente y desempeñar un papel muy diferente en la maquinaria general. Es decir, una demostración de que uno podía entrenar redes profundas pronto se convirtió en la comprensión de que uno debería agregar profundidad a los modelos. En los años siguientes, los grupos de investigación comenzaron a mostrar empírica y teóricamente que la profundidad era importante para aprender eficientemente de los datos (Bengio et al. 2007). La modularidad de una red profunda es clave: cada capa de la estructura funcional desempeña un papel específico, y puede intercambiar capas como bloques de Lego cuando se mueve entre aplicaciones de datos. Esto permite un rápido desarrollo de modelos específicos de la aplicación y también la transferencia del aprendizaje entre modelos: una capa interna de una red que ha sido entrenada para un tipo de problema de reconocimiento de imágenes se puede usar para iniciar una nueva red para un problema diferente. Tarea de visión artificial.\\
El aprendizaje profundo entró en la corriente principal de ML con un artículo de 2012 de Krizhevsky, Sutskever y Hinton (2012) que mostró que su DNN podía superar los puntos de referencia de rendimiento actuales en el conocido concurso de visión por computadora ImageNet. Por ejemplo, las DNN ahora pueden reconocer imágenes y generar subtítulos apropiados (Karpathy y Fei-Fei 2015).\\
Las llamadas redes neuronales convolucionales (CNN; LeCun y Bengio 1995) ilustran la estrategia que hace que el aprendizaje profundo sea tan exitoso: es conveniente apilar capas de diferentes especializaciones de modo que las funciones específicas de la imagen (convoluciones) puedan alimentar capas que son bueno para representar formas funcionales genéricas. Por ejemplo, la arquitectura muy simple que usamos en Hartford et al. (2017) para una tarea que mezclaba reconocimiento de dígitos con datos comerciales (simulados).\\
Para datos de texto, debe incrustar palabras en un espacio vectorial. Esto puede suceder a través de una simple transformación word2vec (Mikolov et al. 2013) (una descomposición lineal en la matriz de conteos de co-ocurrencia para palabras; por ejemplo, dentro de tres palabras entre sí) o a través de una arquitectura LSTM (memoria a largo plazo) (Hochreiter y Schmidhuber 1997): modelos para secuencias de palabras o letras que esencialmente mezclan un modelo oculto de Markov (largo) con un proceso autorregresivo (corto). Y hay muchas otras variantes, con nuevas arquitecturas que se desarrollan todos los días.\\
Una cosa debe quedar clara: hay mucha estructura en los DNN. Estos modelos no son similares a los tipos de modelos de regresión no paramétricos utilizados por estadísticos, econometristas y en ML anterior. Son semiparamétricos. Por lo tanto, las DNN combinan la reducción de dimensiones restrictivas con la aproximación de funciones flexibles. La clave es que ambos componentes se aprenden de forma conjunta.\\
Como advertimos al principio, hemos cubierto solo una pequeña parte del área de aprendizaje profundo. Hay un montón de material nuevo y emocionante que sale tanto de la industria como de la academia. (Para echar un vistazo a lo que está sucediendo en el campo, explore las últimas actas de NIPS [Sistemas de procesamiento de información neuronal, la principal conferencia de ML] https://papers.nips.cc/.\\
Los temas de este tipo están cobrando importancia a medida que las DNN se alejan de las competencias académicas y se acercan a las aplicaciones del mundo real. A medida que el campo crece y la construcción de modelos de DNN pasa de una disciplina científica a una de ingeniería, veremos una mayor necesidad de este tipo de investigación que nos dice cuándo y cuánto podemos confiar en las DNN.


\section{Stochastic Gradient Descent}



