mu <- 5
n <- 10
for (i in 1:M){
x <- rnorm(n,mean=mu,sd=sigma)
var.est[i] <- var(x)
}
mean(x)
x<-rnorm(n,mean=mu,sd=sigma)
x
var(x)
var.est
mean(var.est)
mean(var.est)
################################ Ejercicio 2 ###################################
# Parametros de interes varianza
M <- 10000
var.est <- numeric(M)
var <- 2
mu <- 5
n <- 10
for (i in 1:M){
x <- rnorm(n,mean=mu,sd=sigma)
var.est[i] <- var(x)
}
mean(var.est)
( bias.est <- mean(var.est-var) )
( variance.est <- var(var.est) )
( mse.est <- mean((var.est-var)^2) )
mean(var.est)
################################ Ejercicio 2 ###################################
# Parametros de interes varianza
M <- 10000
var.est <- numeric(M)
var <- 1
mu <- 5
n <- 10
for (i in 1:M){
x <- rnorm(n,mean=mu,sd=sigma)
var.est[i] <- var(x)
}
mean(var.est)
################################ Ejercicio 2 ###################################
# Parametros de interes varianza
M <- 10000
var.est <- numeric(M)
var <- 1
mu <- 0
n <- 10
for (i in 1:M){
x <- rnorm(n,mean=mu,sd=sigma)
var.est[i] <- var(x)
}
mean(var.est)
( bias.est <- mean(var.est-var) )
( variance.est <- var(var.est) )
( mse.est <- mean((var.est-var)^2) )
################################ Ejercicio 2 ###################################
# Parametros de interes varianza
M <- 10000
var.est <- numeric(M)
var <- 2
mu <- 1
n <- 10
for (i in 1:M){
x <- rnorm(n,mean=mu,sd=sigma)
var.est[i] <- var(x)
}
mean(var.est)
################################ Ejercicio 2 ###################################
# Parametros de interes varianza
M <- 10000
var.est <- numeric(M)
var <- 2
mu <- 10
n <- 10
for (i in 1:M){
x <- rnorm(n,mean=mu,sd=sigma)
var.est[i] <- var(x)
}
mean(var.est)
################################ Ejercicio 2 ###################################
# Parametros de interes varianza
M <- 10000
var.est <- numeric(M)
var <- 10
mu <- 10
n <- 10
for (i in 1:M){
x <- rnorm(n,mean=mu,sd=sigma)
var.est[i] <- var(x)
}
mean(var.est)
################################ Ejercicio 2 ###################################
# Parametros de interes varianza
M <- 10000
var.est <- numeric(M)
var <- 1
mu <- 0
n <- 10
for (i in 1:M){
x <- rnorm(n,mean=mu,sd=sigma)
var.est[i] <- var(x)
}
mean(var.est)
( bias.est <- mean(var.est-var) )
( variance.est <- var(var.est) )
( mse.est <- mean((var.est-var)^2) )
hist(var.est)
# Fix the number of Monte Carlo simulations M
M=10000
# Create a vector that will save the values of the estimator
theta.est.1 <- numeric(M)
theta.est.2 <- numeric(M)
theta.est.3 <- numeric(M) # mejor estimador
# Fix the true value of the parameter
theta <- 2
# Fix the sample size (consider sereral values)
n <- 25
# Repeat in a loop M times
for (i in 1:M){
# Step 1.: simulate a sample from a distribution with mean mu
x=runif(n,min=0,max=theta)
# Step 2.: evaluate the estimator in the simulated sample
theta.est.1[i] <- 2*mean(x)   # method of moments estimator
theta.est.2[i] <- max(x)   # maximum likelihood estimator
theta.est.3[i] <- max(x)*(n+1)/n   # maximum likelihood estimator
}
# Bias, variance and MSE:
( bias.est.1 <- mean(theta.est.1-theta) )
( variance.est.1 <- var(theta.est.1) )
( mse.est.1 <- mean((theta.est.1-theta)^2) )
( bias.est.2 <- mean(theta.est.2-theta) )
( variance.est.2 <- var(theta.est.2) )
( mse.est.2 <- mean((theta.est.2-theta)^2) )
#mejor estimador (n-1)/n
( bias.est.3 <- mean(theta.est.3-theta) )
( variance.est.3 <- var(theta.est.3) )
( mse.est.3 <- mean((theta.est.3-theta)^2) )
# Fix the number of Monte Carlo simulations M
M=10000
# Create a vector that will save the values of the estimator
tau.est.1=numeric(M)
tau.est.2=numeric(M)
# Fix the sample size (consider sereral values)
n=50
# Repeat in a loop M times
for (i in 1:M){
# Step 1.: simulate a sample from a distribution with mean mu
# x=rnorm(n); tau=qnorm(0.95)
# x=rt(n,df=5); tau=qt(0.95,df=5)
x=rexp(n,rate=0.5); tau=qexp(0.95,rate=0.5)
# Step 2.: evaluate the estimator in the simulated sample
tau.est.1[i]=quantile(x,0.95)   # non-parametric estimator
tau.est.2[i]=mean(x) + qnorm(0.95)*sd(x)  # parametric estimator
}
# Bias, variance and MSE of the estimators:
( bias.est.1=mean(tau.est.1-tau) )
( variance.est.1=var(tau.est.1) )
( mse.est.1=mean((tau.est.1-tau)^2) )
( bias.est.2=mean(tau.est.2-tau) )
( variance.est.2=var(tau.est.2) )
( mse.est.2=mean((tau.est.2-tau)^2) )
# all values in a row:
round(c(bias.est.1,variance.est.1,mse.est.1,bias.est.2,variance.est.2,mse.est.2),4)
par(mfrow=c(1,2))
hist(tau.est.1,freq=F,xlab="non-parametric estimator",main=paste("n=",n,sep=""))
abline(v=tau,lty=2,col="red",lwd=2)
hist(tau.est.2,freq=F,xlab="parametric estimator",main=paste("n=",n,sep=""))
abline(v=tau,lty=2,col="red",lwd=2)
# Fix the number of Monte Carlo simulations M
M=10000
# Create a vector that will save the values of the estimator
tau.est.1=numeric(M)
tau.est.2=numeric(M)
# Fix the sample size (consider sereral values)
n=400
# Repeat in a loop M times
for (i in 1:M){
# Step 1.: simulate a sample from a distribution with mean mu
# x=rnorm(n); tau=qnorm(0.95)
# x=rt(n,df=5); tau=qt(0.95,df=5)
x=rexp(n,rate=0.5); tau=qexp(0.95,rate=0.5)
# Step 2.: evaluate the estimator in the simulated sample
tau.est.1[i]=quantile(x,0.95)   # non-parametric estimator
tau.est.2[i]=mean(x) + qnorm(0.95)*sd(x)  # parametric estimator
}
# Bias, variance and MSE of the estimators:
( bias.est.1=mean(tau.est.1-tau) )
( variance.est.1=var(tau.est.1) )
( mse.est.1=mean((tau.est.1-tau)^2) )
( bias.est.2=mean(tau.est.2-tau) )
( variance.est.2=var(tau.est.2) )
( mse.est.2=mean((tau.est.2-tau)^2) )
# all values in a row:
round(c(bias.est.1,variance.est.1,mse.est.1,bias.est.2,variance.est.2,mse.est.2),4)
# Fix the number of Monte Carlo simulations M
M=10000
# Create a vector that will save the values of the estimator
tau.est.1=numeric(M)
tau.est.2=numeric(M)
# Fix the sample size (consider sereral values)
n=1000
# Repeat in a loop M times
for (i in 1:M){
# Step 1.: simulate a sample from a distribution with mean mu
# x=rnorm(n); tau=qnorm(0.95)
# x=rt(n,df=5); tau=qt(0.95,df=5)
x=rexp(n,rate=0.5); tau=qexp(0.95,rate=0.5)
# Step 2.: evaluate the estimator in the simulated sample
tau.est.1[i]=quantile(x,0.95)   # non-parametric estimator
tau.est.2[i]=mean(x) + qnorm(0.95)*sd(x)  # parametric estimator
}
# Bias, variance and MSE of the estimators:
( bias.est.1=mean(tau.est.1-tau) )
( variance.est.1=var(tau.est.1) )
( mse.est.1=mean((tau.est.1-tau)^2) )
( bias.est.2=mean(tau.est.2-tau) )
( variance.est.2=var(tau.est.2) )
( mse.est.2=mean((tau.est.2-tau)^2) )
# all values in a row:
round(c(bias.est.1,variance.est.1,mse.est.1,bias.est.2,variance.est.2,mse.est.2),4)
par(mfrow=c(1,2))
hist(tau.est.1,freq=F,xlab="non-parametric estimator",main=paste("n=",n,sep=""))
abline(v=tau,lty=2,col="red",lwd=2)
hist(tau.est.2,freq=F,xlab="parametric estimator",main=paste("n=",n,sep=""))
abline(v=tau,lty=2,col="red",lwd=2)
# Fix the number of Monte Carlo simulations M
M=10000
# Create a vector that will save the values of the estimator
tau.est.1=numeric(M)
tau.est.2=numeric(M)
# Fix the sample size (consider sereral values)
n=400
# Repeat in a loop M times
for (i in 1:M){
# Step 1.: simulate a sample from a distribution with mean mu
# x=rnorm(n); tau=qnorm(0.95)
# x=rt(n,df=5); tau=qt(0.95,df=5)
x=rexp(n,rate=0.5); tau=qexp(0.95,rate=0.5)
# Step 2.: evaluate the estimator in the simulated sample
tau.est.1[i]=quantile(x,0.95)   # non-parametric estimator
tau.est.2[i]=mean(x) + qnorm(0.95)*sd(x)  # parametric estimator
}
# Bias, variance and MSE of the estimators:
( bias.est.1=mean(tau.est.1-tau) )
( variance.est.1=var(tau.est.1) )
( mse.est.1=mean((tau.est.1-tau)^2) )
( bias.est.2=mean(tau.est.2-tau) )
( variance.est.2=var(tau.est.2) )
( mse.est.2=mean((tau.est.2-tau)^2) )
# all values in a row:
round(c(bias.est.1,variance.est.1,mse.est.1,bias.est.2,variance.est.2,mse.est.2),4)
par(mfrow=c(1,2))
# data:
x = c(0.05, 0.07, 0.08, 0.15, 0.18, 0.21, 0.43, 0.48, 0.52, 0.62, 0.65, 0.67, 0.70, 0.81, 0.86, 0.89, 0.91, 0.96, 1.02, 1.05, 1.19, 1.23, 1.44, 1.58, 1.65, 1.79, 1.82, 1.84, 1.93, 1.97, 2.26, 2.27, 2.34, 2.59, 2.76, 3.09, 3.39, 3.54, 3.62, 3.70, 4.28, 4.33, 4.60, 4.69, 4.91, 5.19, 5.35, 6.02, 6.38, 6.76)
hist(x)
mean(x)
sd(x)/sqrt(length(x))
# sample size
n=length(x)
theta.est=mean(x)
# bootstrap algorithm
B=1000
theta.boot=numeric(B)
xboot=sample(x,n,replace=T)  # bootstrap resample X1*,...,X2*
theta.boot[b]=mean(xboot)    # bootstrap estimation
# bootstrap algorithm
B=1000
theta.boot=numeric(B)
sample(x,n,replace=T)
for (b in 1:B){
xboot=sample(x,n,replace=T)  # bootstrap resample X1*,...,X2*
theta.boot[b]=mean(xboot)    # bootstrap estimation
}
# estimation of the bias the estimator or theta:
( bias.boot=mean(theta.boot)-theta.est)
# approximation of the standard error of the estimator or theta
( se.boot=sd(theta.boot) )
hist(theta.boot)
sd(x)/sqrt(length(x))
# approximation of the standard error of the estimator or theta
( se.boot=sd(theta.boot) )
hist(theta.boot)
# data:
x = c(0.05, 0.07, 0.08, 0.15, 0.18, 0.21, 0.43, 0.48, 0.52, 0.62, 0.65, 0.67, 0.70, 0.81, 0.86, 0.89, 0.91, 0.96, 1.02, 1.05, 1.19, 1.23, 1.44, 1.58, 1.65, 1.79, 1.82, 1.84, 1.93, 1.97, 2.26, 2.27, 2.34, 2.59, 2.76, 3.09, 3.39, 3.54, 3.62, 3.70, 4.28, 4.33, 4.60, 4.69, 4.91, 5.19, 5.35, 6.02, 6.38, 6.76)
hist(x)
mean(x)
sd(x)/sqrt(length(x))
# sample size
n=length(x)
theta.est=mean(x)
# bootstrap algorithm
B=1000
theta.boot=numeric(B)
for (b in 1:B){
xboot=sample(x,n,replace=T)  # bootstrap resample X1*,...,X2*
theta.boot[b]=mean(xboot)    # bootstrap estimation
}
# estimation of the bias the estimator or theta:
( bias.boot=mean(theta.boot)-theta.est)
# approximation of the standard error of the estimator or theta
( se.boot=sd(theta.boot) )
hist(theta.boot)
################### estimación bootstrap para la mediana #######################
x = c(0.05, 0.07, 0.08, 0.15, 0.18, 0.21, 0.43, 0.48, 0.52, 0.62, 0.65, 0.67, 0.70, 0.81, 0.86, 0.89, 0.91, 0.96, 1.02, 1.05, 1.19, 1.23, 1.44, 1.58, 1.65, 1.79, 1.82, 1.84, 1.93, 1.97, 2.26, 2.27, 2.34, 2.59, 2.76, 3.09, 3.39, 3.54, 3.62, 3.70, 4.28, 4.33, 4.60, 4.69, 4.91, 5.19, 5.35, 6.02, 6.38, 6.76)
median(x)
################### estimación bootstrap para la mediana #######################
x = c(0.05, 0.07, 0.08, 0.15, 0.18, 0.21, 0.43, 0.48, 0.52, 0.62, 0.65, 0.67, 0.70, 0.81, 0.86, 0.89, 0.91, 0.96, 1.02, 1.05, 1.19, 1.23, 1.44, 1.58, 1.65, 1.79, 1.82, 1.84, 1.93, 1.97, 2.26, 2.27, 2.34, 2.59, 2.76, 3.09, 3.39, 3.54, 3.62, 3.70, 4.28, 4.33, 4.60, 4.69, 4.91, 5.19, 5.35, 6.02, 6.38, 6.76)
n=length(x)
theta.est=median(x)
################### estimación bootstrap para la mediana #######################
x = c(0.05, 0.07, 0.08, 0.15, 0.18, 0.21, 0.43, 0.48, 0.52, 0.62, 0.65, 0.67, 0.70, 0.81, 0.86, 0.89, 0.91, 0.96, 1.02, 1.05, 1.19, 1.23, 1.44, 1.58, 1.65, 1.79, 1.82, 1.84, 1.93, 1.97, 2.26, 2.27, 2.34, 2.59, 2.76, 3.09, 3.39, 3.54, 3.62, 3.70, 4.28, 4.33, 4.60, 4.69, 4.91, 5.19, 5.35, 6.02, 6.38, 6.76)
n=length(x)
theta.est=median(x)
# bootstrap algorithm
B = 1000
theta.boot <- numeric(B)
for (b in 1:B){
xboot <- sample(x,n,replace = T)
theta.boot[b] <- median(xboot)
}
( bias.boot = median(theta.boot) - theta.est )
( bias.boot = median(theta.boot) - theta.est )
( se.boot <- sd(theta.boot) )
sd(x)/sqrt(length(x))
x = c(0.05, 0.07, 0.08, 0.15, 0.18, 0.21, 0.43, 0.48, 0.52, 0.62, 0.65, 0.67, 0.70, 0.81, 0.86, 0.89, 0.91, 0.96, 1.02, 1.05, 1.19, 1.23, 1.44, 1.58, 1.65, 1.79, 1.82, 1.84, 1.93, 1.97, 2.26, 2.27, 2.34, 2.59, 2.76, 3.09, 3.39, 3.54, 3.62, 3.70, 4.28, 4.33, 4.60, 4.69, 4.91, 5.19, 5.35, 6.02, 6.38, 6.76)
hist(x)
mean(x)
sd(x)/sqrt(length(x))
# sample size
n=length(x)
theta.est=mean(x)
# bootstrap algorithm
B=1000
theta.boot=numeric(B)
for (b in 1:B){
xboot=sample(x,n,replace=T)  # bootstrap resample X1*,...,X2*
theta.boot[b]=mean(xboot)    # bootstrap estimation
}
# estimation of the bias the estimator or theta:
( bias.boot=mean(theta.boot)-theta.est)
# approximation of the standard error of the estimator or theta
( se.boot=sd(theta.boot) )
hist(theta.boot)
sd(x)/sqrt(length(x))
# approximation of the standard error of the estimator or theta
( se.boot=sd(theta.boot) )
n=length(x)
theta.est=median(x)
sd(x)/sqrt(length(x))
# bootstrap algorithm
B = 1000
theta.boot <- numeric(B)
for (b in 1:B){
xboot <- sample(x,n,replace = T)
theta.boot[b] <- median(xboot)
}
( bias.boot = median(theta.boot) - theta.est )
( se.boot <- sd(theta.boot) )
x = c(0.05, 0.07, 0.08, 0.15, 0.18, 0.21, 0.43, 0.48, 0.52, 0.62, 0.65, 0.67, 0.70, 0.81, 0.86, 0.89, 0.91, 0.96, 1.02, 1.05, 1.19, 1.23, 1.44, 1.58, 1.65, 1.79, 1.82, 1.84, 1.93, 1.97, 2.26, 2.27, 2.34, 2.59, 2.76, 3.09, 3.39, 3.54, 3.62, 3.70, 4.28, 4.33, 4.60, 4.69, 4.91, 5.19, 5.35, 6.02, 6.38, 6.76)
n=length(x)
theta.est=median(x)
sd(x)/sqrt(length(x))
# bootstrap algorithm
B = 1000
theta.boot <- numeric(B)
for (b in 1:B){
xboot <- sample(x,n,replace = T)
theta.boot[b] <- median(xboot)
}
( bias.boot = median(theta.boot) - theta.est )
( se.boot <- sd(theta.boot) )
hist(theta.boot)
x = c(0.05, 0.07, 0.08, 0.15, 0.18, 0.21, 0.43, 0.48, 0.52, 0.62, 0.65, 0.67, 0.70, 0.81, 0.86, 0.89, 0.91, 0.96, 1.02, 1.05, 1.19, 1.23, 1.44, 1.58, 1.65, 1.79, 1.82, 1.84, 1.93, 1.97, 2.26, 2.27, 2.34, 2.59, 2.76, 3.09, 3.39, 3.54, 3.62, 3.70, 4.28, 4.33, 4.60, 4.69, 4.91, 5.19, 5.35, 6.02, 6.38, 6.76)
n=length(x)
theta.est=median(x)
sd(x)/sqrt(length(x))
# bootstrap algorithm
B = 1000
theta.boot <- numeric(B)
for (b in 1:B){
xboot <- sample(x,n,replace = T)
theta.boot[b] <- median(xboot)
}
( bias.boot = median(theta.boot) - theta.est )
( se.boot <- sd(theta.boot) )
hist(theta.boot)
x = c(0.05, 0.07, 0.08, 0.15, 0.18, 0.21, 0.43, 0.48, 0.52, 0.62, 0.65, 0.67, 0.70, 0.81, 0.86, 0.89, 0.91, 0.96, 1.02, 1.05, 1.19, 1.23, 1.44, 1.58, 1.65, 1.79, 1.82, 1.84, 1.93, 1.97, 2.26, 2.27, 2.34, 2.59, 2.76, 3.09, 3.39, 3.54, 3.62, 3.70, 4.28, 4.33, 4.60, 4.69, 4.91, 5.19, 5.35, 6.02, 6.38, 6.76)
n=length(x)
theta.est=median(x)
sd(x)/sqrt(length(x))
# bootstrap algorithm
B = 1000
theta.boot <- numeric(B)
for (b in 1:B){
xboot <- sample(x,n,replace = T)
theta.boot[b] <- median(xboot)
}
( bias.boot = median(theta.boot) - theta.est )
( se.boot <- sd(theta.boot) )
hist(theta.boot)
x = c(0.05, 0.07, 0.08, 0.15, 0.18, 0.21, 0.43, 0.48, 0.52, 0.62, 0.65, 0.67, 0.70, 0.81, 0.86, 0.89, 0.91, 0.96, 1.02, 1.05, 1.19, 1.23, 1.44, 1.58, 1.65, 1.79, 1.82, 1.84, 1.93, 1.97, 2.26, 2.27, 2.34, 2.59, 2.76, 3.09, 3.39, 3.54, 3.62, 3.70, 4.28, 4.33, 4.60, 4.69, 4.91, 5.19, 5.35, 6.02, 6.38, 6.76)
n=length(x)
theta.est=median(x)
sd(x)/sqrt(length(x))
# bootstrap algorithm
B = 1000
theta.boot <- numeric(B)
for (b in 1:B){
xboot <- sample(x,n,replace = T)
theta.boot[b] <- median(xboot)
}
( bias.boot = median(theta.boot) - theta.est )
( se.boot <- sd(theta.boot) )
hist(theta.boot)
x = c(0.05, 0.07, 0.08, 0.15, 0.18, 0.21, 0.43, 0.48, 0.52, 0.62, 0.65, 0.67, 0.70, 0.81, 0.86, 0.89, 0.91, 0.96, 1.02, 1.05, 1.19, 1.23, 1.44, 1.58, 1.65, 1.79, 1.82, 1.84, 1.93, 1.97, 2.26, 2.27, 2.34, 2.59, 2.76, 3.09, 3.39, 3.54, 3.62, 3.70, 4.28, 4.33, 4.60, 4.69, 4.91, 5.19, 5.35, 6.02, 6.38, 6.76)
n=length(x)
theta.est=median(x)
sd(x)/sqrt(length(x))
# bootstrap algorithm
B = 1000
theta.boot <- numeric(B)
for (b in 1:B){
xboot <- sample(x,n,replace = T)
theta.boot[b] <- median(xboot)
}
( bias.boot = median(theta.boot) - theta.est )
( se.boot <- sd(theta.boot) )
hist(theta.boot)
x = c(0.05, 0.07, 0.08, 0.15, 0.18, 0.21, 0.43, 0.48, 0.52, 0.62, 0.65, 0.67, 0.70, 0.81, 0.86, 0.89, 0.91, 0.96, 1.02, 1.05, 1.19, 1.23, 1.44, 1.58, 1.65, 1.79, 1.82, 1.84, 1.93, 1.97, 2.26, 2.27, 2.34, 2.59, 2.76, 3.09, 3.39, 3.54, 3.62, 3.70, 4.28, 4.33, 4.60, 4.69, 4.91, 5.19, 5.35, 6.02, 6.38, 6.76)
n=length(x)
theta.est=median(x)
sd(x)/sqrt(length(x))
# bootstrap algorithm
B = 1000
theta.boot <- numeric(B)
for (b in 1:B){
xboot <- sample(x,n,replace = T)
theta.boot[b] <- median(xboot)
}
( bias.boot = median(theta.boot) - theta.est )
( se.boot <- sd(theta.boot) )
hist(theta.boot)
x = c(0.05, 0.07, 0.08, 0.15, 0.18, 0.21, 0.43, 0.48, 0.52, 0.62, 0.65, 0.67, 0.70, 0.81, 0.86, 0.89, 0.91, 0.96, 1.02, 1.05, 1.19, 1.23, 1.44, 1.58, 1.65, 1.79, 1.82, 1.84, 1.93, 1.97, 2.26, 2.27, 2.34, 2.59, 2.76, 3.09, 3.39, 3.54, 3.62, 3.70, 4.28, 4.33, 4.60, 4.69, 4.91, 5.19, 5.35, 6.02, 6.38, 6.76)
n=length(x)
theta.est=median(x)
sd(x)/sqrt(length(x))
# bootstrap algorithm
B = 1000
theta.boot <- numeric(B)
for (b in 1:B){
xboot <- sample(x,n,replace = T)
theta.boot[b] <- median(xboot)
}
( bias.boot = median(theta.boot) - theta.est )
( se.boot <- sd(theta.boot) )
hist(theta.boot)
x = c(0.05, 0.07, 0.08, 0.15, 0.18, 0.21, 0.43, 0.48, 0.52, 0.62, 0.65, 0.67, 0.70, 0.81, 0.86, 0.89, 0.91, 0.96, 1.02, 1.05, 1.19, 1.23, 1.44, 1.58, 1.65, 1.79, 1.82, 1.84, 1.93, 1.97, 2.26, 2.27, 2.34, 2.59, 2.76, 3.09, 3.39, 3.54, 3.62, 3.70, 4.28, 4.33, 4.60, 4.69, 4.91, 5.19, 5.35, 6.02, 6.38, 6.76)
n=length(x)
theta.est=median(x)
sd(x)/sqrt(length(x))
# bootstrap algorithm
B = 1000
theta.boot <- numeric(B)
for (b in 1:B){
xboot <- sample(x,n,replace = T)
theta.boot[b] <- median(xboot)
}
( bias.boot = median(theta.boot) - theta.est )
( se.boot <- sd(theta.boot) )
hist(theta.boot)
median(x)
theta.est=quantile(x,.9)
########################### cuantil 0.9 ########################################
x = c(0.05, 0.07, 0.08, 0.15, 0.18, 0.21, 0.43, 0.48, 0.52, 0.62, 0.65, 0.67, 0.70, 0.81, 0.86, 0.89, 0.91, 0.96, 1.02, 1.05, 1.19, 1.23, 1.44, 1.58, 1.65, 1.79, 1.82, 1.84, 1.93, 1.97, 2.26, 2.27, 2.34, 2.59, 2.76, 3.09, 3.39, 3.54, 3.62, 3.70, 4.28, 4.33, 4.60, 4.69, 4.91, 5.19, 5.35, 6.02, 6.38, 6.76)
n=length(x)
theta.est=quantile(x,.9)
median(x)
sd(x)/sqrt(length(x))
# bootstrap algorithm
B = 1000
theta.boot <- numeric(B)
x = c(0.05, 0.07, 0.08, 0.15, 0.18, 0.21, 0.43, 0.48, 0.52, 0.62, 0.65, 0.67, 0.70, 0.81, 0.86, 0.89, 0.91, 0.96, 1.02, 1.05, 1.19, 1.23, 1.44, 1.58, 1.65, 1.79, 1.82, 1.84, 1.93, 1.97, 2.26, 2.27, 2.34, 2.59, 2.76, 3.09, 3.39, 3.54, 3.62, 3.70, 4.28, 4.33, 4.60, 4.69, 4.91, 5.19, 5.35, 6.02, 6.38, 6.76)
n=length(x)
theta.est=quantile(x,.9)
median(x)
sd(x)/sqrt(length(x))
# bootstrap algorithm
B = 1000
theta.boot <- numeric(B)
for (b in 1:B){
xboot <- sample(x,n,replace = T)
theta.boot[b] <- quantile(xboot,.9)
}
( bias.boot = median(theta.boot) - theta.est )
( se.boot <- sd(theta.boot) )
hist(theta.boot)
# t-student la t de estudent es poner un peso en la campana de la normal
qnorm(0.95)
qt(.95,df=9)
qnorm(0.95)
qt(.95,df=9)
qt(.95,df=19)
qt(.95,df=999)
qnorm(0.95)
qt(.95,df=9)
qt(.95,df=19)
qt(.95,df=999)
qnorm(0.95)
qt(.95,df=999)
qt(0.975,df = 9)
qnorm(0.975)
