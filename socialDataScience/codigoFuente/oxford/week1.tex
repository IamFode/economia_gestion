\part{Equidad y sesgo algorítmicos}

\chapter{Preservación del sesgo en el aprendizaje automático: La legalidad de las métricas de imparcialidad según la Ley de no discriminación de la UE (Watchet, Mittlestadt y Russel. 2021)}
Medimos una gran mayoría de algorítmicos en términos de producción de tendencias históricas. Este tipo de precisión suele ser la única medida del rendimiento. Pero ¿es justo este enfoque?. \\

En los últimos años se trabajó mucho para abordar el sesgo en el aprendizaje automático y los sistemas de IA, donde muchos estudios instan mayor responsabilidad en su diseño y uso. Nos referiremos a dos categorías importantes de sesgos problemáticos a los que nos referiremos como:

\begin{enumerate}[1.]
    \item Sesgo técnico.
    \item Sesgo social.
\end{enumerate}

Los sesgos técnicos reflejan un fallo de los algoritmos de aprendizaje a la hora de predecir con la misma precisión, en distintos escenarios. Ahora, el sesgo social es una cuestión de política, perspectivas y cambios de prejuicios e ideas preconcebidas que puede tardar décadas en cambiar. Cabe esperar resultados sesgados cuando los sistemas se entrenan con datos que reflejan fielmente la realidad social, es decir, que captan los sesgos y las desigualdades que caracterizan a las sociedades modernas. \\

Las soluciones técnicas en el aprendizaje automático justo, son sólo una solución temporal para los síntomas, pero no para las causas de la desigualdad de la sociedad.\\

Al final nos quedan tres soluciones para el sesgo algorítmicos y la desigualdad histórica social:

\begin{enumerate}[1.]	
    \item No hacer nada.
    \item Rectificar los sesgos técnicos y mantener el statu quo.
    \item Reconocer el hecho de que el statu quo a menudo no es neutral y, en su lugar, utilizar la IA y el análisis estadístico para arrojar luz sobre las desigualdades existentes.
\end{enumerate}

Hasta la fecha gran parte del trabajo se centró en la segunda opción, dado métricas que se ajustan a la igualdad formal que pretenden reproducir el rendimiento histórico. Lamentablemente, el uso de estas métricas corre el riesgo de desviar la atención de las causas subyacentes de las desigualdades históricas y, por tanto, puede desviar la atención de su solución. La tercera opción está relacionada con las métricas de imparcialidad (transformadoras de sesgo), donde se parte de puntos diferentes que no son iguales.\\

Este documento realiza tres aportaciones. En primer lugar, distinguimos entre dos posibles objetivos fundamentales de la legislación en materia de no discriminación, la igualdad formal y la igualdad sustantiva, que imponen diferentes obligaciones a los desarrolladores, implantadores y usuarios de la IA, el aprendizaje automático y la toma de decisiones automatizada. En segundo lugar, proponemos un sistema de clasificación de las métricas de imparcialidad en el aprendizaje automático basado en la forma en que gestionan el sesgo preexistente (métricas de imparcialidad que "preservan el sesgo" y métricas de imparcialidad que "transforman el sesgo") y en qué medida se ajustan a los objetivos de la legislación en materia de no discriminación. Por último, reconocemos que la necesidad legal de justificación en casos de discriminación indirecta puede crear nuevas obligaciones para desarrolladores, implantadores y usuarios. Reconociendo esta necesidad de justificación, sostenemos que las métricas que exigen una elección explícita de los sesgos que debe heredar un clasificador deberían ser preferibles a efectos de una toma de decisiones justa en el marco de la igualdad sustantiva. Para concluir, ofrecemos recomendaciones concretas sobre cómo elegir la métrica de equidad más adecuada en virtud de la legislación de la UE en materia de no discriminación y una lista de comprobación para hacerlo.

\section{Igualdad formal y sustantiva en el derecho antidiscriminatorio}

La legislación de la UE en materia de no discriminación prohíbe dos tipos:

\begin{enumerate}[1.]
    \item Discriminación directa.
    \item Discriminación indirecta.
\end{enumerate}

Discriminación directa significa que una persona recibe un trato menos favorable en función de un atributo protegido (por ejemplo, raza y etnia,  género, religión y creencias, edad, discapacidad u orientación sexual) que posee en asuntos de un sector protegido (por ejemplo, el lugar de trabajo, la provisión de bienes y servicios). Se basa en el postulado Aritotélico de tratar casos similares de forma similar y casos diferentes de forma diferente, también se denomina igualdad formal o principio del mérito.\\

La igualdad formal no garantiza la igualdad de oportunidades. Para lograr esta igualdad primero es necesario reconocer que existe una desigualdad estructural generalizada, luego se debe fortalecer la capacidad de luchar por esas oportunidades. No solo se busca la igualdad jurídica, sino la capacidad humana, no sólo la igualdad como derecho y como teoría, sino la igualdad como hecho y resultado.\\

Proporcionar a las personas igualdad de acceso a las oportunidades (es decir, igualdad formal) no equivale a proporcionar un acceso ajustado a las disparidades históricas y a sus efectos duraderos sobre los grupos protegidos. Esta última, denominada "igualdad sustantiva" de oportunidades (o "igualdad de facto "), no puede lograrse simplemente ignorando los atributos protegidos (por ejemplo, raza, sexo, discapacidad) y tratando a todos por igual en adelante. Se requiere una actitud más activa que tenga en cuenta las realidades sociales e históricas.\\

La igualdad sustantiva se centra en medidas positivas que nivelan el terreno de juego para mejorar la competencia leal con el fin de cuestionar los criterios de acceso establecidos, esto a diferencia de la igualdad de oportunidades formal el cual se centra en los aspectos procedimentales de la igualdad en la asignación de recursos.


\subsection{Discriminación indirecta e igualdad sustantiva}
El concepto de discriminación indirecta se creó para lograr la igualdad sustantiva en la práctica. La discriminación indirecta ayuda a desmantelar las estructuras de poder subyacentes así como a identificar las áreas en las que es necesario adoptar medidas adicionales para lograr una verdadera igualdad. La discriminación indirecta pretende ayudar a redistribuir los recursos de los favorecidos a los desfavorecidos y promover la diversidad en la sociedad. \\

La discriminación indirecta justificada se produce cuando el presunto infractor persigue un objetivo legítimo y los mecanismos utilizados superan la "prueba de proporcionalidad", lo que significa que son tanto legalmente necesarios como proporcionados. Por ejemplo, los requisitos físicos pueden justificarse como esenciales a la hora de contratar bomberos por motivos de seguridad, aunque impongan una desventaja particular.\\

La discriminación indirecta difiere de la directa en que reconoce que deben tenerse en cuenta los obstáculos sociales, las luchas y las diferencias de hecho a las que se enfrentan los grupos protegidos.65 La discriminación indirecta reconoce las diferencias entre grupos y postula que deben recibir un trato diferente.

\subsection{Acción positiva e igualdad sustantiva}
La acción positiva, aunque también es una forma de igualdad sustantiva, se centra únicamente en la igualdad de resultados. Aquí es donde difiere la igualdad de oportunidades sustantiva: pretende crear procedimientos justos utilizando criterios de toma de decisiones que tengan en cuenta las desigualdades históricas. El objetivo no es simplemente dar una ventaja a determinados miembros de un grupo desfavorecido dándoles un mejor resultado. Más bien, la igualdad de oportunidades sustantiva pretende crear unas condiciones equitativas para todos los participantes definiendo los procedimientos y criterios de toma de decisiones teniendo en cuenta las desigualdades históricas (por ejemplo, no basarse en gran medida en las cartas de recomendación o el promedio de notas).


\section{Preservación de sesgo en el aprendizaje automático justo}
La adecuación de las capacidades técnicas para medir el sesgo y la desigualdad con el objetivo y los deberes asoaiados a la legislación en materia de no discriminación es de vital importancia para los desarrolladores y usuarios del aprendizaje automático y la IA.\\

En esta sección proponemos un esquema de clasificación para las métricas de imparcialidad en el aprendizaje automático basado en la distinción jurídica fundamental entre igualdad formal y sustantiva. En concreto, consideramos que ciertos sesgos sociales en la toma de decisiones en el pasado son problemáticos debido a la desigualdad que han creado entre grupos protegidos de personas en la sociedad. A partir de esta observación, argumentamos que preservar estos sesgos en los modelos de aprendizaje automático puede ser problemático. Si se rechazara el argumento de que la desigualdad existente es de hecho un problema, entonces se podría rechazar igualmente el argumento de que preservar ese sesgo en el aprendizaje automático justo es problemático.

\subsection{Métricas de equidad y derecho antidiscriminatorio}
En particular, algunas de las pruebas utilizadas por el Tribunal de Justicia de las Comunidades Europeas y los tribunales de los Estados miembros para medir la discriminación indirecta coinciden con la métrica de la paridad demográfica de la equidad algorítmica.\\
Formalmente, la definición de paridad demográfica afirma que cada grupo protegido, es decir, un grupo basado en un atributo protegido como la raza o el sexo, debería, si recibe el k\% de las decisiones positivas, recibir también el k\% de las decisiones negativas.\\

Se dice que un sistema de toma de decisiones presenta independencia condicional con respecto a un atributo protegido concreto, como la raza o el sexo, y a un atributo condicionante, como el salario o la antigüedad en el empleo, si:

\begin{enumerate}[1.]
    \item Cualquier diferencia en la forma en que el sistema trata colectivamente a las personas de una determinada raza o sexo puede atribuirse enteramente a diferencias en el atributo condicionante; y 
    \item tras condicionar esta variable, las decisiones tomadas son estadísticamente independientes del grupo protegido.
\end{enumerate}

Imaginamos un escenario hipotético en el que una determinada aplicación algorítmica de toma de decisiones es impugnada ante un tribunal por causar discriminación indirecta. En este contexto, hay que responder a dos preguntas clave: 

\begin{enumerate}[1.]
	\item ¿Existe una disparidad significativa? 
	\item Aceptando que existe una disparidad significativa, ¿está justificada?
\end{enumerate}

Lo ideal sería que los desarrolladores y usuarios del aprendizaje automático respondieran proactivamente a estas preguntas en el momento de su implantación, con la vista puesta en futuras responsabilidades, pero también para demostrar un compromiso con la igualdad sustantiva y no meramente formal.\\
La primera cuestión se refiere a cómo definimos los grupos pertinentes (es decir, los grupos desfavorecidos y de comparación) y cómo medimos la disparidad entre ellos. Por lo que respecta a la segunda, la primera pregunta puede reformularse como sigue: ¿qué métrica de equidad debemos utilizar para medir la disparidad? Y, más concretamente, ¿a qué variable o variables debe condicionarse la prueba? Se trata de una cuestión normativa clave, ya que la respuesta puede dar lugar a que se ignoren o se oculten desigualdades potencialmente problemáticas, sobre todo cuando son el resultado de prejuicios y desigualdades del pasado.

\subsection{Métricas de equidad que preservan y transforman el sesgo}
Para ayudar a responder a estas preguntas en el contexto de un marco jurídico diseñado para la igualdad sustantiva, definimos dos tipos de métricas de equidad. Las métricas de imparcialidad que "preservan el sesgo" tratan de reproducir el rendimiento histórico en los resultados del modelo objetivo con índices de error equivalentes para cada grupo, tal y como se refleja en los datos de entrenamiento (o statu quo). Por el contrario, las métricas que "transforman el sesgo" no aceptan ciegamente el sesgo social como un punto de partida dado o neutral que debe preservarse, sino que requieren que las personas tomen una decisión explícita sobre qué sesgos debe mostrar el sistema.\\

Para formalizar nuestra noción de imparcialidad que preserva el sesgo, decimos que cualquier métrica de imparcialidad preserva el sesgo si siempre es satisfecha por un clasificador perfecto que predice exactamente sus etiquetas objetivo con error cero, replicando el sesgo presente en los datos. Las métricas de equidad que no son necesariamente satisfechas por un clasificador perfecto se denominan transformadoras del sesgo.\\

La métrica de equidad en cuestión, las probabilidades igualadas se definen formalmente como: 

"el predictor $\hat{Y}$ satisface las probabilidades iquiponderadas con respecto al atributo protegido $A$ y al resultado $Y$ si $\hat{Y}$ y $A$ son independientes, condicionado a $Y$,"  donde $\hat{Y}$ es la salida de un sistema, e $Y$, algunas etiquetas de entrada que el sistema está tratando de predecir.

Basándose en esta definición, las probabilidades igualadas son una forma de independencia condicional o paridad demográfica condicional, condicionada a los datos históricos Y, y que refleja exactamente sus sesgos. Todas estas métricas tienen en común la idea de que el sesgo presente en los datos de las etiquetas objetivo tiene que estar ahí, y un clasificador perfecto que reproduzca exactamente las etiquetas dadas (es decir, Y = $\hat{Y}$) satisfaría todas esas métricas. Como tales, todas estas métricas deberían entenderse como un intento de evitar que los sistemas de aprendizaje automático inserten nuevos sesgos en un sistema preservando el sesgo presente en los datos. Nos referimos a tales métricas de imparcialidad como "preservadoras del sesgo". \\

\subsection{Límites del sesgo que preserva y transforma las métricas}
La diferencia clave entre las métricas de transformación de sesgos y las de preservación de sesgos es que la mayoría de las métricas de transformación de sesgos se satisfacen igualando las tasas de decisión entre grupos, mientras que las métricas de preservación de sesgos suelen requerir igualar las tasas de error entre grupos.\\

Las métricas que preservan el sesgo suelen igualar la tasa de error entre grupos. Por ejemplo, las probabilidades equiponderadas requieren que la proporción de verdaderos positivos y falsos negativos sea la misma en todos los grupos, y que la proporción de verdaderos negativos y falsos positivos también sea la misma en todos los grupos.\\

En el campo del aprendizaje automático, en la práctica es habitual entrenar un sistema con variables sustitutivas que son más fáciles de medir que las variables que queremos que el sistema prediga. Por ejemplo, un sistema puede entrenarse para predecir si una persona tiene una puntuación crediticia alta como variable sustitutiva de si devolverá un préstamo, o un sistema puede entrenarse para predecir si una persona será detenida como variable sustitutiva de si infringirá la ley. Este desajuste entre lo que queremos predecir y las variables sustitutivas que podemos observar es otra forma de que el sesgo sistemático se introduzca en los sistemas. Este tipo de sesgo social heredado no puede detectarse mediante el uso ingenuo de métricas de preservación del sesgo que simplemente miden si el sistema de aprendizaje automático recupera valores de las variables proxy con errores similares para cada grupo.\\


\section{El status quo no es neutral}

Teniendo esto en cuenta, las métricas de imparcialidad son potencialmente problemáticas por varios motivos. Muchas de sus limitaciones se deben a que consideran el statu quo como un punto de partida neutral para evaluar la equidad en el aprendizaje automático. Estas métricas no diferencian entre las razones de la desigualdad en el pasado, sino que sólo importa la reproducción del rendimiento histórico con tasas de error comparables para cada grupo. Se considera "justo" simplemente igualar estos porcentajes de error. Por consiguiente, ignoran los sesgos y desigualdades sociales subyacentes en un contexto de toma de decisiones determinado.138 En cambio, las métricas de transformación de sesgos requieren una elección normativa positiva con respecto a qué sesgos debería mostrar el sistema de toma de decisiones. Al hacer esta elección, cualquier caso reconocido de disparidad entre grupos puede considerarse potencialmente discriminatorio y necesitado de justificación jurídica.\\

Ignorar las razones que subyacen a la desigualdad es problemático desde el punto de vista de la igualdad sustantiva, ya que comprender por qué se tomaron las decisiones históricamente es crucial para corregir las desigualdades que crearon.\\

Tratar el statu quo como neutral no reconoce suficientemente esta realidad social. En otras palabras, al limitarse a tratar de preservar las tasas de error por grupo, los métodos de preservación del sesgo sobrevaloran implícitamente el papel de la meritocracia. Esto puede ser problemático; en las sociedades occidentales, factores como la herencia, la suerte, la desigualdad de oportunidades y la discriminación son tan importantes para el éxito como el mérito.168 Por ejemplo, el mejor indicador para predecir si una persona será pobre de adulta es si nació en la pobreza.\\

Para evaluar y justificar potencialmente la discriminación prima facie en el aprendizaje automático justo, es esencial reconocer la diversa manifestación de desigualdades que se producen en todo el mundo. La discriminación racial y de género y otras cuestiones de intolerancia en los Estados Unidos y en los Estados miembros de la UE se manifestarán de forma diferente según los legados culturales e históricos de cada país. No se puede asumir fácilmente que un tipo concreto de desigualdad también se da en otros entornos. Lo que hemos acuñado como "igualdad contextual" debe tenerse en cuenta a la hora de elegir y justificar las métricas de equidad y los sesgos heredados en el aprendizaje automático y la IA.


\section{Hacia la igualdad sustantiva en el aprendizaje automático}
Como se ha demostrado anteriormente, el statu quo está marcado por importantes sesgos y desigualdades implícitos y explícitos. Utilizar decisiones pasadas como base para futuras decisiones automatizadas significa que los sesgos del pasado pueden ser heredados fácilmente por un modelo entrenado.\\

Por definición, los modelos de alta precisión entrenados con datos históricos para satisfacer una métrica de preservación del sesgo a menudo replicarán el sesgo presente en sus datos de entrenamiento.\\

Las métricas de transformación de sesgos pueden considerarse "medios menos lesivos" porque son más adecuadas para promover la igualdad sustantiva. En concreto, a diferencia de las métricas que preservan el sesgo, ofrecen al responsable de la toma de decisiones la posibilidad de elegir las propiedades que debe presentar un clasificador. Lo hacen mediante la elección de la variable o variables condicionantes para la independencia condicional, o la elección de la métrica para la imparcialidad a través de la concienciación. 176 Al hacerlo, permiten seleccionar una métrica menos intrusiva o sesgada como base para las decisiones. En consecuencia, los argumentos ofrecidos para justificar el uso de métricas que preservan el sesgo podrían fracasar, ya que podrían no considerarse "necesarias" en un sentido jurídico, a menos que pueda demostrarse que el condicionamiento de la variable objetivo es el medio menos intrusivo posible para lograr un interés legítimo. Con esto no se pretende sugerir que el uso de métricas de transformación de sesgos en la toma de decisiones automatizada eliminará los sesgos históricos o evitará por completo futuras disparidades.\\

Las métricas de transformación de sesgos obligan a los diseñadores y responsables de la toma de decisiones a enfrentarse a la imparcialidad y a considerar los sesgos y desigualdades de sus datos que, de otro modo, serían ignorados, ocultados o tratados como justificados por las métricas de preservación de sesgos.\\

Cuando se consideran como una herramienta para hacer frente a las disparidades del pasado, las métricas de transformación de sesgos tienen dos ventajas claras. En primer lugar, en el contexto de los litigios, las métricas de transformación de sesgos obligan a sacar a la luz desigualdades que, de otro modo, se pasarían por alto. En segundo lugar, debatir la disparidad y la relativa intrusividad de los posibles criterios de equidad ofrece a los responsables de la toma de decisiones o a los desarrolladores de sistemas automatizados la oportunidad de ajustar el proceso de toma de decisiones y los criterios para igualar las condiciones de los grupos desfavorecidos, a menos que pueda demostrarse una justificación legal de la disparidad. Cuando se utilizan correctamente, las métricas de transformación de sesgos ayudan a garantizar que la desigualdad en la toma de decisiones automatizada se reconozca explícitamente, se discuta y se justifique potencialmente de forma coherente y realista. Este tipo de autorreflexión crítica y previsión es esencial para que los usuarios de la IA, el aprendizaje automático y la toma de decisiones automatizada en los sectores público y privado desempeñen un papel más activo en el desmantelamiento de la desigualdad.\\

\textbf{Para avanzar activamente hacia la igualdad sustantiva en el aprendizaje automático justo, recomendamos el uso de métricas que transformen el sesgo para la toma de decisiones.} Lo ideal sería que los usuarios probaran lo más ampliamente posible tanto las métricas de preservación de sesgos como las de transformación para investigar la imparcialidad de sus sistemas de toma de decisiones.

\section{Conclusiones y recomendaciones}
El aprendizaje automático justo se rige predominantemente por medidas estadísticas de justicia y correcciones que abordan el "sesgo técnico". Este enfoque ignora importantes decisiones normativas explícitas sobre cómo debe comportarse un sistema y corre el riesgo de dejar importantes decisiones jurídicas, éticas y políticas exclusivamente en manos de los desarrolladores, implantadores y usuarios. Estas decisiones determinan lo que es justo y discriminatorio, si una "desventaja particular" es lo suficientemente grave como para justificar un debate y, en última instancia, si la discriminación indirecta puede justificarse.\\

Existe un nuevo esquema de clasificación de las métricas de equidad para aclarar las líneas de debate y dejar claras las dimensiones normativas y políticas del trabajo técnico sobre el aprendizaje automático equitativo. Los desarrolladores pueden elegir entre dos tipos de métricas: 
\begin{enumerate}[1.]
    \item Métricas de "preservación del sesgo" que toman la sociedad tal y como existe actualmente como un punto de partida neutral o un "campo de juego nivelado" desde el que podemos medir la desigualdad y el sesgo en el aprendizaje automático; y 
    \item métricas de "transformación del sesgo" que reconocen las desigualdades históricas y parten del supuesto de que ciertos grupos tendrán un punto de partida peor que otros.
\end{enumerate}

\subsection{Una lista de verificación para elegir métricas de equidad apropiadas}
El sesgo que preserva la métrica en la toma de decisiones puede dar lugar a una discriminación indirecta prima facie (a primera vista). El uso de criterios de equidad no es una elección neutral. 
Para ayudar en este proceso de elección de métricas de equidad apropiadas tanto para fines de diagnóstico como de toma de decisiones en el aprendizaje automático, se presenta una lista de comprobación:

\begin{enumerate}[P1]
    \item ¿Utiliza los parámetros de equidad únicamente para diagnosticar la disparidad pero no se toman decisiones sustanciales sobre los individuos?\\\\
	\begin{tabular}{rl}
	    Sí:& se pueden utilizar tanto métricas que preserven el sesgo como métricas transformadoras.\\
	    No:& pase a la pregunta 2.
	\end{tabular}
    \item ¿Está desplegando un sistema para tomar decisiones en un ámbito con una desigualdad social histórica inaceptable?\\\\
	\begin{tabular}{rl}
	    Sí:& pase a la pregunta 3.\\
	    No:& Se recomienda investigar el posible sesgo en el caso de uso antes de elegir una métrica. En los \\
	       & casos en los que desigualdad histórica no existe, o la disparidad conocida se ha legalmente \\
	       & justificada, se pueden utilizar tanto métricas de pueden utilizarse tanto las métricas que \\
	       & preservan el sesgo como las que lo transforman.
	\end{tabular}
    \item ¿Está desplegando el sistema y en una jurisdicción legal que promueve únicamente la igualdad formal?\\\\
	\begin{tabular}{rl}
	    Si: & Se pueden utilizar tanto métricas que preserven el sesgo como métricas transformadoras.\\
	    No: & pase a la pregunta 4.
	\end{tabular}
	\item ¿Está desplegando el sistema y en una jurisdicción legal que promueve la igualdad sustantiva?\\\\
	    \begin{tabular}{rl}
		Sí: & se recomienda utilizar una métrica de transformación de sesgos.\\
		No: & se pueden utilizar tanto métricas de preservación como de transformación del sesgo.
	    \end{tabular}
\end{enumerate}

\subsection{Uso de métricas transformadoras de sesgos para apoyar la igualdad sustantiva}
La Disparidad Demográfica Condicional (DDC), un tipo de métrica de transformación de la independencia condicional y el sesgo, es la métrica de equidad más compatible con los conceptos de igualdad y disparidad ilegal desarrollados por el Tribunal de Justicia de las Comunidades Europeas.187 Esta compatibilidad confiere una mayor legitimidad jurídica al uso de la métrica por parte de agentes públicos y privados para medir el sesgo y la equidad en la IA y los sistemas algorítmicos de toma de decisiones.\\

La CDD trata a todas las personas (grupos) como iguales, lo que significa que deben recibir el mismo trato. La prueba señala cualquier disparidad entre grupos que persista una vez aplicada una variable condicionante adecuada. Esta noción de equidad sigue el postulado aristotélico de tratar "casos iguales" y permite la igualdad formal. Por supuesto, la DDC tiene limitaciones y no es una solución milagrosa para la equidad algorítmica.\\

La DDC y otras métricas de transformación del sesgo permiten así a los agentes públicos y privados desempeñar un papel más activo en el establecimiento de la igualdad sustantiva. Cuando se detecta una disparidad injustificada, puede ser necesario adaptar los procesos, por ejemplo cambiando los criterios de decisión, añadiendo variables diferentes o dando un peso distinto a las ya existentes (por ejemplo, diciendo a un modelo que dé menos importancia al salario o a las interrupciones de carrera porque son indicadores del rendimiento laboral sesgados por el género).\\

Las métricas de transformación de sesgos pueden ayudar a identificar a los candidatos con talento infravalorados por criterios de decisión sesgados que no reflejan de forma coherente y justa el mérito y la competencia de todos los candidatos.

\subsection{Deberes sustantivos de igualdad en el aprendizaje automático equitativo}
Especificar los requisitos de los deberes positivos de igualdad en el aprendizaje automático justo es un área importante para la investigación futura.\\

El diálogo sobre qué sesgos debe adoptar un sistema, sobre qué variables debe condicionar (en el caso de la independencia condicional) y qué formas de desigualdad pueden justificarse es clave para promover la igualdad sustantiva en la práctica.\\

Las métricas de transformación de sesgos y las estadísticas resumidas pueden considerarse una hoja de ruta para el cambio social en el lugar de trabajo, los préstamos, la educación, la justicia penal, la sanidad, los seguros y otros ámbitos.

\subsection{Más datos por si solos no son la solución}
Por último, el sesgo a menudo no se produce por razones técnicas, sino porque un conjunto de datos no es representativo de la población. La desigualdad tiene muchas y diversas caras. No se puede suponer que una forma de intolerancia (por ejemplo, en Estados Unidos) también exista o se manifieste de la misma manera en otro lugar (por ejemplo, Alemania). Se necesitan más datos para investigar la desigualdad multifacética a nivel mundial y promover lo que hemos acuñado en otro lugar como "igualdad contextual".\\

Se podría caer en la tentación de pensar que los problemas de parcialidad e imparcialidad en el aprendizaje automático se resolverán de forma natural recopilando más datos (sensibles) y cerrando las brechas de representación. Sin embargo, es ingenuo suponer que la recopilación de más datos dará lugar necesariamente a resultados justos y equitativos.\\

Para justificar una mayor recogida y uso de datos sensibles, es necesario demostrar primero un compromiso serio y la voluntad política de rectificar la desigualdad. 

Un primer paso para demostrar este compromiso en la práctica es el cumplimiento proactivo de los deberes positivos en torno a la igualdad sustantiva. La elección de mantener el statu quo mediante el uso de métricas de imparcialidad que preserven el sesgo no puede considerarse una opción neutral a este respecto; más bien, debe entenderse como una opción jurídicamente significativa que requiere una consideración explícita por parte de los desarrolladores, usuarios y reguladores de IA en el futuro.


\chapter{Matices de género: Disparidades de exactitud interseccional en clasificación comercial de género (Buolamwini, Gebru, 2018)}

\section{Introducción}
La Inteligencia Artificial (IA) se está infiltrando rápidamente en todos los aspectos de la sociedad. Desde ayudar a determinar a quién se contrata, a quién se despide, a quién se le concede un préstamo o cuánto tiempo pasa una persona en prisión, las decisiones que tradicionalmente han sido tomadas por humanos son rápidamente tomadas por algoritmos. Recientemente se ha demostrado que los algoritmos entrenados con datos sesgados han dado lugar a discriminación algorítmica (Bolukbasi et al., 2016; Caliskan et al., 2017). De forma similar a los efectos perjudiciales bien documentados de los ensayos clínicos sesgados (Popejoy y Fullerton, 2016; Melloni et al., 2010), las muestras sesgadas en IA para la atención sanitaria pueden dar lugar a tratamientos que no funcionen bien para muchos segmentos de la población.\\
Una investigación de un año de duración en 100 departamentos de policía reveló que las personas afroamericanas tienen más probabilidades de ser detenidas por las fuerzas del orden y de ser sometidas a registros de reconocimiento facial que las personas de otras etnias (Garvie et al., 2016). Los falsos positivos y los registros injustificados suponen una amenaza para las libertades civiles. Se ha demostrado que algunos sistemas de reconocimiento facial identifican erróneamente a personas de color, mujeres y jóvenes en porcentajes elevados (Klare et al., 2012). Es necesario supervisar la precisión fenotípica y demográfica de estos sistemas, así como su uso, para proteger los derechos de los ciudadanos.\\

En lugar de evaluar la precisión únicamente por género o tipo de piel, también se examina la precisión en 4 subgrupos interseccionales: mujeres más morenas, hombres más morenos, mujeres más claras y hombres más claros. Los 3 clasificadores de género comerciales evaluados tienen la precisión más baja en las mujeres más morenas. Dado que la tecnología de visión por ordenador se utiliza en sectores de alto riesgo, como la sanidad y el orden público, es necesario seguir trabajando en la evaluación comparativa de algoritmos de visión para diversos grupos demográficos y fenotípicos.

\section{Trabajos relacionados}
\subsection{Análisis facial automatizado}
El análisis facial automatizado se ha utilizado para la detección y clasificación de rostros.\\

Affectiva e investigadores del mundo académico intentan identificar emociones a partir de imágenes de rostros de personas. (Dehghan et al., 2017; Srinivasan et al., 2016; Fabian Benitez-Quiroz et al., 2016). Algunos trabajos también han utilizado el análisis facial automatizado para comprender y ayudar a las personas con autismo (Leo et al., 2015; Palestra et al., 2016). Trabajos controvertidos como (Kosinski y Wang, 2017) afirman determinar la sexualidad de varones caucásicos cuyas fotos de perfil están en Facebook o en sitios de citas. Y otros como (Wu y Zhang, 2016) y la empresa con sede en Israel Faception (Faception) han desarrollado un software que pretende determinar las características de un individuo (por ejemplo, propensión a la delincuencia, coeficiente intelectual, terrorismo) únicamente a partir de sus rostros. \\

El último informe sobre clasificación por género del Instituto Nacional de Normalización y Tecnología (NIST) también muestra que los algoritmos evaluados por el NIST obtienen peores resultados con rostros etiquetados como femeninos que con rostros etiquetados como masculinos (Ngan et al., 2015).

\section{Parámetro intersectional}
\subsection{Justificación del etiquetado fenotípico}
Aunque las etiquetas raciales son adecuadas para evaluar la posible discriminación algorítmica en algunas formas de datos (por ejemplo, los utilizados para predecir las tasas de reincidencia criminal), se enfrentan a dos limitaciones clave cuando se utilizan en imágenes visuales. En primer lugar, las características fenotípicas de los sujetos pueden variar mucho dentro de una misma categoría racial o étnica. Por ejemplo, los tipos de piel de los individuos que se identifican como negros en Estados Unidos pueden presentar muchas tonalidades. Por tanto, los puntos de referencia del análisis facial formados por individuos negros de piel más clara no representarían adecuadamente a los de piel más oscura. En segundo lugar, las categorías raciales y étnicas no son homogéneas en toda la geografía: incluso dentro de un mismo país, estas categorías cambian con el tiempo.\\

Al etiquetar los rostros con el tipo de piel, podemos comprender mejor el rendimiento de este importante atributo fenotípico.


\subsection{Justificación de la selección del índice de referencia existente}

Anotamos el conjunto de datos con el sistema Fitzpatrick de clasificación de la piel y probamos el rendimiento de la clasificación de género en 4 subgrupos: mujeres más oscuras, hombres más oscuros, mujeres más claras y hombres más claros. En general, todos los clasificadores obtuvieron mejores resultados con los individuos y los hombres más claros. Los peores resultados se obtuvieron con las mujeres más morenas. Es necesario seguir trabajando para comprobar si las diferencias sustanciales en la tasa de error en función del sexo, el tipo de piel y el subgrupo interseccional reveladas en este estudio de clasificación por sexo persisten en otras tareas de visión artificial basadas en humanos.\\

Dado que la equidad algorítmica se basa en diferentes supuestos contextuales y optimizaciones de la precisión, este trabajo pretende demostrar por qué necesitamos informes rigurosos sobre las métricas de rendimiento en las que se centran los debates sobre equidad algorítmica.


\chapter{La injusticia del aprendizaje automático imparcial: Nivelación a la baja e igualitarismo estricto por defecto (Mittelstadt, Wachter, Rusell. 2023)}

Este artículo examina críticamente estas observaciones iniciales para determinar cómo el MLJ puede ir más allá de la mera nivelación a la baja y del igualitarismo estricto por defecto. Examinamos las causas y la prevalencia de la nivelación a la baja en fairML, y exploramos posibles justificaciones y críticas basadas en teorías filosóficas y jurídicas de la igualdad y la justicia distributiva, así como en la jurisprudencia en materia de igualdad. 

\section{Introducción}
En los últimos años, la equidad en el aprendizaje automático, la inteligencia artificial y los sistemas algorítmicos de toma de decisiones se ha convertido en un área muy activa de investigación y desarrollo. Los usos previstos y reales de estas tecnologías para distribuir resultados y recursos en ámbitos de alto riesgo como la medicina, el derecho y las finanzas han suscitado un interés justificado por la equidad y la parcialidad de estas decisiones. El despliegue de estas tecnologías ha sido cuestionado por los responsables políticos y los investigadores 6 debido a la percepción de falta de fiabilidad y equidad. \\

El campo del aprendizaje automático justo (fairML) ha sido impulsado principalmente por investigadores y profesionales que trabajan en ML e IA, informática, ingeniería de software y matemáticas. Estos grupos han desarrollado numerosas medidas y métodos para mitigar el sesgo y mejorar la equidad en los sistemas algorítmicos. Sin embargo, la mayoría de estas herramientas se han creado al margen de los contextos políticos y de la sociedad civil y carecen de un compromiso serio con las teorías filosóficas, políticas, jurídicas y económicas de la igualdad y la justicia distributiva.\\

Se ha demostrado que muchas de las medidas de equidad actuales adolecen tanto de falta de equidad como de pérdida de rendimiento, o "nivelación a la baja", en la que la equidad se consigue empeorando la situación de todos los grupos o rebajando el rendimiento de los grupos con mejores resultados al nivel de los grupos con peores resultados.\\

La nivelación a la baja es un síntoma de la decisión de medir la equidad únicamente en términos de igualdad, o disparidad entre grupos en cuanto a rendimiento y resultados, ignorando otras características relevantes de la justicia distributiva, como el bienestar absoluto o la prioridad, que son más difíciles de cuantificar y medir directamente en entornos de investigación y desarrollo.\\

la nivelación a la baja en fairML sugiere que el campo está adoptando, intencionadamente o no, un estricto enfoque igualitario de las cuestiones de justicia distributiva en el que el único valor (medible) es la igualdad. Denominamos a estas tendencias en fairML "igualitarismo estricto por defecto". El igualitarismo estricto por defecto, es contrario tanto a los objetivos declarados de las medidas de equidad como al objetivo presunto del campo: mejorar los resultados de los grupos históricamente desfavorecidos o marginados.  Concibe la igualdad en términos comparativos simplistas, ignorando los absolutos de bienestar y justicia que son necesarios para lograr la igualdad sustantiva en lugar de la mera igualdad formalista, o la igualdad de trato.\\

Cuando la equidad sólo se logra empeorando la situación de los demás grupos, parece que algo a fallado a la hora de trasladar a la práctica el vago concepto de "equidad". La igualdad debe aspirar a que las personas estén mejor, no simplemente a derucirla a un nivel común de prejuicio. plicar la equidad únicamente mediante la nivelación a la baja desperdicia esta oportunidad de lograr una igualdad sustantiva en lugar de meramente formalista. \\


\section{Nivelación}
Los métodos que aplican medidas de equidad basadas en la equidad de grupo son problemáticos desde el punto de vista moral y legal debido a

\begin{enumerate}[(1)]
    \item sus valores implícitos, que favorecen desproporcionadamente la igualdad frente a la reducción de daño, y
    \item la forma en que logran la igualdad en la práctica mediante la nivelación a la baja, por  lo que ciertos grupos empeoran innecesariamente en aras de la conveniencia matemática.
\end{enumerate}
A grandes rasgos, podemos descomponer el aprendizaje automático (AM) justo en dos componentes: 
\begin{itemize}
    \item MEDIDAS.- Son cálculo sencillos, como la diferencia en la tasa de verdaderos positivos entre dos grupos.
    \item MÉTODOS.- Son enfoques novedosos del ML que mejoran estas medidas, igualando las tasas de daño a expensas de criterios como la precisión general, que normalmente optimiza un sistema de ML sin tener en cuenta la imparcialidad.
\end{itemize}

Cuando construimos sistemas de ML para tomar decisiones sobre la vida de las personas, nuestras decisiones de diseño codifican juicios de valor implícitos sobre qué propiedades deben ser priorizadas por el comportamiento del sistema. La idea de que la precisión no siempre es la propiedad más relevante para evaluar el rendimiento de un modelo es comúnmente aceptada en la investigación del ML.\\


\subsection{Nivelación a la baja mediante equidad de grupo}
Una vez seleccionada una propiedad importante para la igualdad puede aplicarse de dos maneras:
\begin{itemize}
    \item Ajustar el rendimiento en la propiedad elegida para el grupo desfavorecido,
    \item Degradar el rendimiento para los grupos favorecidos en la misma propiedad.
\end{itemize}

Imponer la igualdad reduciendo el rendimiento de los grupos favorecidos provoca el fenómeno que denominamos "nivelación a la baja". La nivelación a la baja se produce cuando la igualdad se impone no sólo aumentando la propiedad relevante para los grupos desfavorecidos, sino también empeorando arbitrariamente la situación de uno o más grupos con mejores resultados al reducir su rendimiento en la misma propiedad, cómo un sentido ineficiente de Pareto.\\

La pérdida de rendimiento de los grupos favorecidos no es estrictamente necesaria para mejorar el rendimiento, la tasa de decisión o alguna otra propiedad valiosa para los grupos desfavorecidos, sino que se inflige únicamente para reducir la disparidad de rendimiento en la propiedad elegida, satisfaciendo así una definición matemática de justicia que dice que un modelo es justo cuando este número es igual entre los grupos. La nivelación a la baja no beneficia directamente a los grupos con peores resultados.\\


\subsection{Ejemplo: Nivelación a la baja en el diagnóstico de cáncer}
e ha demostrado que las tecnologías de detección del cáncer de pulmón y de piel son menos precisas para las personas de piel más oscura, lo que significa que con mayor frecuencia no detectan los cánceres en los pacientes, retrasando el acceso a la atención que salva vidas. Una forma de mejorar la situación de los pacientes negros es, por tanto, mejorar la capacidad de recuerdo del sistema. Como primer paso, podemos decidir pecar de precavidos y decirle al sistema que cambie sus predicciones para los casos en los que confía menos en que se trate de pacientes negros. En concreto, cambiaríamos algunos casos de "bajo riesgo" de baja confianza a "alto riesgo" para detectar más casos de cáncer en el futuro. Este cambio se produce a costa de la precisión: aumenta el número de personas clasificadas incorrectamente como de riesgo de cáncer y disminuye la precisión global del sistema. Sin embargo, esta compensación entre precisión y memoria es aceptable porque no predecir un futuro caso de cáncer puede perjudicar gravemente a los pacientes. De lo que hemos hecho todo lo posible para mejorar la situación de los pacientes negros y reducir la diferencia de rendimiento. Segundo, podemos seguir reduciendo el rendimiento de los pacientes blancos, disminuyendo tanto su recuerdo como su precisión en el proceso, de forma que nuestro sistema funcione igual de bien, o lo más parecido posible, para ambos grupos.\\

Esto es lo que hacen en la práctica muchos métodos de equidad de grupo. Evidentemente, este tipo de cambio de etiquetas en aras de la igualdad puede ser extremadamente perjudicial para los pacientes a los que no se ofrecería atención de seguimiento y control. La precisión global disminuye y la frecuencia del tipo de error más perjudicial aumenta, todo por reducir la diferencia de rendimiento. Así pues, la nivelación a la baja no beneficia directamente a ninguno de los dos grupos. 

\section{¿Es habitual reducir el nivel en fairML?}
El uso de medidas de equidad basadas en la igualdad en el aprendizaje automático no es evidentemente problemático, sino que es la forma en que se aplican en la práctica, y la nivelación a la baja resultante, lo que causa problemas. La nivelación a la baja puede producirse como resultado directo del uso de medidas igualitarias en la selección de modelos. En muchos sentidos, este problema es otro ejemplo de la Ley de Goodhart, según la cual "cuando una medida se convierte en un objetivo, deja de ser una buena medida".\\

Demostramos cómo se produce la nivelación a la baja para una serie de medidas de equidad, centrándonos en dos de las métricas más utilizadas en la literatura sobre fairML: la paridad demográfica y la igualdad de oportunidades.


\subsection{Nivelar hacia abajo en la teoría}
Las medidas de equidad se inspiran implícitamente, o se derivan explícitamente, de las teorías de la justicia distributiva. La justicia distributiva se refiere al "impacto relativo de las asignaciones en los diferentes grupos sociales o subgrupos de la población, dadas las desigualdades sociales existentes".\\

Según el igualitarismo estricto, es aceptable empeorar la situación de un grupo sin beneficiar directamente a otros para eliminar la disparidad.  Las medidas que combinan la justicia con una noción estricta de igualdad y trato equitativo dominan actualmente la literatura sobre fairML.  Como era de esperar, se ha observado una tendencia a lograr la igualdad a través de la nivelación hacia abajo para las medidas de equidad de grupo independientemente de su fundamento teórico igualitario específico. \\

Parece, por tanto, que la aplicación de la equidad de grupo respalda el igualitarismo estricto, aunque inadvertidamente debido a cómo se mide la paridad y no a una elección teórica intencionada. Desde la perspectiva de la justicia distributiva, esta tendencia es muy preocupante.

\section{¿Es justificable la nivelación a la baja?}
\subsection{El valor de la igualdad}
Para los igualitaristas estrictos, la igualdad tiene un valor intrínseco, lo que significa que la igualdad es buena en sí misma. Del mismo modo, la desigualdad es mala en sí misma, aunque no tenga efectos negativos. Se puede decir que la igualdad tiene un valor instrumental derivado de los buenos efectos que produce. Es buena como medio para alcanzar algún otro objetivo valioso relacionado con la justicia, como la libertad universal, el desarrollo de las capacidades y la personalidad humanas, la mitigación del sufrimiento y la estigmatización, o evitar el conflicto o la envidia entre las personas creados por la desigualdad.\\

La desigualdad puede ser instrumentalmente mala, por ejemplo, por la injusticia social que conlleva. Hay que considerar dos tipos de injusticia: 
\begin{enumerate}[(1)]
    \item Un sentido comparativo de la justicia, lo que significa que estamos preocupados por el mero hecho de que las personas son tratadas de forma diferente a los demás, o no reciben su parte justa de un recurso; o 
    \item un sentido no comparativo de la justicia, donde la injusticia surge porque una persona no es tratada como se merece, independientemente de cualquier consideración sobre cómo son tratados los demás.
\end{enumerate}
En términos de nivelación a la baja, un sentido comparativo de la justicia por sí solo no rechazaría la eliminación de la disparidad al tratar a todos de forma neutral pero igual de injusta, mientras que un sentido no comparativo la rechazaría sobre la base de que las personas son tratadas injustamente en términos absolutos, por ejemplo al negárseles una parte de un recurso vital.

\section{Conclusiones}
La nivelación a la baja es un síntoma de la elección de medir la equidad únicamente en términos de disparidad entre grupos y asumir un valor uniforme para los beneficios y los perjuicios, ignorando el bienestar, la prioridad y otros bienes, así como la estigmatización, la preocupación desigual, la pérdida de solidaridad y otros perjuicios sustantivos que son fundamentales para las cuestiones de igualdad en el mundo real. Esta no es una solución satisfactoria para los problemas de justicia distributiva en la IA y el ML.\\

Mejorar sustancialmente el rendimiento del clasificador, en comparación con la nivelación a la baja, puede ser difícil, consumir tiempo y recursos, puede requerir nuevos datos y no siempre es posible para los sistemas bien diseñados. No obstante, la nivelación a la baja no es el destino inevitable de la aplicación de la equidad, sino el resultado de tomar el camino más fácil por conveniencia matemática, y no por razones sociales, legales o éticas generales. La equidad no puede seguir tratándose como un simple problema matemático.\\

De cara al futuro, vemos tres posibles caminos para fairML: 

\begin{enumerate}[1.]
    \item Podemos seguir desplegando sistemas sesgados que beneficien ostensiblemente sólo a un segmento privilegiado de la población mientras perjudican a otros.
    \item Podemos seguir definiendo la equidad en términos matemáticos formalistas y desplegar sistemas de IA y ML que funcionen peor para todos los grupos y perjudiquen activamente a algunos grupos. 
    \item Podemos tomar medidas y lograr la equidad mediante la "nivelación", es decir, diseñar sistemas que generen deliberadamente más falsos positivos para grupos (históricamente) desfavorecidos y dedicar los recursos adicionales necesarios para hacerles un seguimiento más frecuente (por ejemplo, aumentar la frecuencia de las pruebas de detección del cáncer).
\end{enumerate}

Este es el reto para el futuro de la equidad en la IA: crear sistemas que sean sustancialmente justos mediante la nivelación ascendente, y no sólo procedimentalmente justos mediante la nivelación descendente. Se trata de un reto mucho más complejo que simplemente ajustar un sistema para que dos números sean iguales entre grupos. Puede requerir no sólo una importante innovación tecnológica y metodológica, incluido el rediseño de los sistemas de IA desde cero, sino también cambios sociales sustanciales en ámbitos como el acceso a la atención sanitaria y el gasto sanitario.\\

Hemos creado métodos matemáticamente justos, pero que no benefician a los más desfavorecidos. Lo que necesitamos son sistemas que sean justos a través de la nivelación, que ayuden a los grupos (históricamente) desfavorecidos sin perjudicar arbitrariamente a los demás. Este es el reto que fairML debe resolver ahora. Necesitamos una IA que sea sustancialmente justa, no sólo matemáticamente.


\chapter{Equidad y abstracción en los sistemas sociotécnicos (Selbst, Boyd, Friedler, Venkatasubramanian, Vertesi. 2019)}
